{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline using Title and Body Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from toolbox.data_prep_helpers import *\n",
    "from toolbox.evaluation import *\n",
    "\n",
    "from models.lstm_classifier import create_model\n",
    "from models.title_body_lstm import create_model as tb_create_model\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pythonquestions/\"\n",
    "ft_path = \"alldata_sg.ft\"  # set this to None if you want to train your own fasttext embeddings\n",
    "n_top_labels = 100\n",
    "n_epochs = 30\n",
    "max_question_words = 100\n",
    "sample_size = -1  # set to -1 to use entire data\n",
    "normalize_embeddings = True\n",
    "use_titles = False\n",
    "\n",
    "tokenized_field = \"q_all_body_tokenized\"\n",
    "content_field = \"Body_q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dschr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from cached pickle\n",
      "reading chunk 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(607282, 5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(data_path, ignore_cache=False, tokenized_field=tokenized_field, content_field=content_field)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-63dd98a0f552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msample_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "sample = df.sample(sample_size) if sample_size > 0 else df\n",
    "del df\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_html_tags(chunk, [\"Body_q\"])\n",
    "#print(f\"{i}: generating question level tokens\")\n",
    "sample[\"q_title_tokenized\"] = sample[\"Title\"].apply(generate_question_level_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(607282, 6)\n",
      "(606841, 6)\n",
      "deleting element python from top_tags\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(425658, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have some nans in our tags which break target encoding\n",
    "print(sample.shape)\n",
    "sample = sample[sample[\"tags\"].apply(lambda tags: all([isinstance(t, str) for t in tags]))]\n",
    "print(sample.shape)\n",
    "\n",
    "\n",
    "# Reduce the number of tags and adjust dataframe accordingly\n",
    "sample = reduce_number_of_tags(sample, n_top_labels)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [osx]\n",
       "1    [windows, image]\n",
       "3     [sql, database]\n",
       "4            [arrays]\n",
       "5       [django, oop]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"tags\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78101, 6)\n",
      "(19526, 6)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text into words on question level\n",
    "data = sample[sample[tokenized_field].apply(len) <= max_question_words]\n",
    "data = data[data[\"q_all_body_tokenized\"].apply(len) > 0]\n",
    "\n",
    "# train_data, test_data = train_test_split(data, test_size = 0.2)\n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body_q</th>\n",
       "      <th>tags</th>\n",
       "      <th>q_all_body_tokenized</th>\n",
       "      <th>q_title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>384010</td>\n",
       "      <td>29941228</td>\n",
       "      <td>Evaluation of sympy.function containing a NumP...</td>\n",
       "      <td>My problem is that the following code doesn't ...</td>\n",
       "      <td>[numpy]</td>\n",
       "      <td>[my, problem, is, that, the, following, code, ...</td>\n",
       "      <td>[evaluation, of, sympy.function, containing, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458459</td>\n",
       "      <td>33716181</td>\n",
       "      <td>ca_certs_locater/__init__.py import error</td>\n",
       "      <td>I was trying to get authentication of my api.H...</td>\n",
       "      <td>[api]</td>\n",
       "      <td>[i, was, trying, to, get, authentication, of, ...</td>\n",
       "      <td>[ca_certs_locater/__init__.py, import, error]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48052</td>\n",
       "      <td>5299199</td>\n",
       "      <td>Python - Importing a global/site-packages modu...</td>\n",
       "      <td>I'm using python and virtualenv/pip. I have a ...</td>\n",
       "      <td>[django, import]</td>\n",
       "      <td>[i, 'm, using, python, and, virtualenv/pip, i,...</td>\n",
       "      <td>[python, -, importing, a, global/site-packages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186264</td>\n",
       "      <td>17527949</td>\n",
       "      <td>Open windows photo gallery from python</td>\n",
       "      <td>I want the end of a python script to open wind...</td>\n",
       "      <td>[windows]</td>\n",
       "      <td>[i, want, the, end, of, a, python, script, to,...</td>\n",
       "      <td>[open, windows, photo, gallery, from, python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282620</td>\n",
       "      <td>24044330</td>\n",
       "      <td>SQLAlchemy query filter behavior confusing in ...</td>\n",
       "      <td>I am a little confused at how to use multiple ...</td>\n",
       "      <td>[mysql, sqlalchemy]</td>\n",
       "      <td>[i, am, a, little, confused, at, how, to, use,...</td>\n",
       "      <td>[sqlalchemy, query, filter, behavior, confusin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "384010  29941228  Evaluation of sympy.function containing a NumP...   \n",
       "458459  33716181          ca_certs_locater/__init__.py import error   \n",
       "48052    5299199  Python - Importing a global/site-packages modu...   \n",
       "186264  17527949             Open windows photo gallery from python   \n",
       "282620  24044330  SQLAlchemy query filter behavior confusing in ...   \n",
       "\n",
       "                                                   Body_q  \\\n",
       "384010  My problem is that the following code doesn't ...   \n",
       "458459  I was trying to get authentication of my api.H...   \n",
       "48052   I'm using python and virtualenv/pip. I have a ...   \n",
       "186264  I want the end of a python script to open wind...   \n",
       "282620  I am a little confused at how to use multiple ...   \n",
       "\n",
       "                       tags  \\\n",
       "384010              [numpy]   \n",
       "458459                [api]   \n",
       "48052      [django, import]   \n",
       "186264            [windows]   \n",
       "282620  [mysql, sqlalchemy]   \n",
       "\n",
       "                                     q_all_body_tokenized  \\\n",
       "384010  [my, problem, is, that, the, following, code, ...   \n",
       "458459  [i, was, trying, to, get, authentication, of, ...   \n",
       "48052   [i, 'm, using, python, and, virtualenv/pip, i,...   \n",
       "186264  [i, want, the, end, of, a, python, script, to,...   \n",
       "282620  [i, am, a, little, confused, at, how, to, use,...   \n",
       "\n",
       "                                        q_title_tokenized  \n",
       "384010  [evaluation, of, sympy.function, containing, a...  \n",
       "458459      [ca_certs_locater/__init__.py, import, error]  \n",
       "48052   [python, -, importing, a, global/site-packages...  \n",
       "186264      [open, windows, photo, gallery, from, python]  \n",
       "282620  [sqlalchemy, query, filter, behavior, confusin...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I am using the Photoshop's javascript API to find the fonts in a given PSD\", 'Given a font name returned by the API I want to find the actual physical font file that that font name corresponds to on the disc']\n",
      "['i', 'am', 'using', 'the', 'photoshop', \"'s\", 'javascript', 'api', 'to', 'find', 'the', 'fonts', 'in', 'a', 'given', 'psd']\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode characters in position 1-14: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-ddff15f66202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_fasttext_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_FastText_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_field\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Dokumente\\Studium\\TUM\\Kurse\\Informatik\\Semester 5\\ADNLP\\so_nlp\\toolbox\\data_prep_helpers.py\u001b[0m in \u001b[0;36mcreate_FastText_embeddings\u001b[1;34m(dataframe, textcolumn)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m         retval = super(FastText, self).build_vocab(\n\u001b[0;32m    724\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m             keep_raw_vocab=keep_raw_vocab, trim_rule=trim_rule, **kwargs)\n\u001b[0m\u001b[0;32m    726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m--> 942\u001b[1;33m         \u001b[0mreport_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'memory'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_retained_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mestimate_memory\u001b[1;34m(self, vocab_size, report)\u001b[0m\n\u001b[0;32m    773\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompatible_hash\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m                     )\n\u001b[0;32m    777\u001b[0m                     \u001b[0mnum_ngrams\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36mft_ngram_hashes\u001b[1;34m(word, minn, maxn, num_buckets, fb_compatible)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfb_compatible\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mencoded_ngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_ngrams_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mhashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mft_hash_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnum_buckets\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\_utils_any2vec.pyx\u001b[0m in \u001b[0;36mgensim.models._utils_any2vec.compute_ngrams_bytes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\_utils_any2vec.pyx\u001b[0m in \u001b[0;36mgensim.models._utils_any2vec.compute_ngrams_bytes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'utf-8' codec can't encode characters in position 1-14: surrogates not allowed"
     ]
    }
   ],
   "source": [
    "# train word embeddings ONLY with training data\n",
    "# wv = create_Word2Vec_embeddings(train_data, \"Body_q\")\n",
    "# Use FastText to include solution for out-of-vocab words\n",
    "if ft_path is not None:\n",
    "    wv = load_fasttext_embeddings(ft_path)\n",
    "else:\n",
    "    wv = create_FastText_embeddings(df, content_field)   \n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"embedding_sg.ft\", \"wb\") with out_file:\n",
    "    pickle.dump(wv, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78101, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Title and Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3e1ad1ce797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"q_title_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"q_all_body_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpadding_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X_t = data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_b = data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "padding_element = np.array([0.0] * X_train_t.iloc[0].shape[-1])\n",
    "\n",
    "X_t = pad_sequences(X_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_b = pad_sequences(X_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_t.shape)\n",
    "print(X_b.shape)\n",
    "\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "label_encoder.fit(df[\"tags\"])\n",
    "y = label_encoder.transform(df[\"tags\"])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78101, 39, 100)\n",
      "(78101, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_t = train_data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_train_b = train_data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "\n",
    "padding_element = np.array([0.0] * X_train_t.iloc[0].shape[-1])\n",
    "\n",
    "X_train_t_padded = pad_sequences(X_train_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_train_b_padded = pad_sequences(X_train_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_train_t_padded.shape)\n",
    "print(X_train_b_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19526, 40, 100)\n",
      "(19526, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "X_test_t = test_data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_test_b = test_data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "X_test_t_padded = pad_sequences(X_test_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_test_b_padded = pad_sequences(X_test_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_test_t_padded.shape)\n",
    "print(X_test_b_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = MultiLabelBinarizer()\n",
    "label_encoder.fit(train_data[\"tags\"])\n",
    "y_train = label_encoder.transform(train_data[\"tags\"])\n",
    "y_test = label_encoder.transform(test_data[\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.training import grid_search_es\n",
    "\n",
    "search_params = {\n",
    "    # conduct big grid search with these params\n",
    "    # \"lstm_layer_size\": [256, 128],\n",
    "    # \"lstm_dropout\": [0.0, 0.2, 0.4],\n",
    "    # \"num_mid_dense\": [1, 0],\n",
    "    \n",
    "    # test grid search with these params (comment out for actual run)\n",
    "    \"lstm_layer_size\": [16],\n",
    "    \"lstm_dropout\": [0.0],\n",
    "    \"num_mid_dense\": [1, 0]\n",
    "    \n",
    "    # don't change these:\n",
    "    \"output_dim\": [y.shape[-1]]\n",
    "}\n",
    "\n",
    "all_hists = grid_search_es(create_model, search_params)\n",
    "\n",
    "best_params, best_hist, best_loss = min(all_hists, key=lambda x: x[2])\n",
    "\n",
    "epoch_lengths = [len(h[\"val_loss\"]) for h in best_hist]\n",
    "print(f\"best combindation: {best_params}\")\n",
    "print(f\"avg min val_loss: {best_loss} -- epoch counts: {epoch_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 100)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          365568      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          51300       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 782,436\n",
      "Trainable params: 782,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tb_create_model(**best_params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir=\"logs/fit/\" + model_name\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    ModelCheckpoint(filepath=f\"checkpoints/{model_name}\", monitor=\"val_loss\", restore_best_weights=True, verbose=0)\n",
    "]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, validation_data=[X_test, y_test], callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict([X_test_t_padded, X_test_b_padded])\n",
    "\n",
    "l_pred = label_encoder.inverse_transform(binarize_model_output(predictions, threshold=0.10))\n",
    "l_true = label_encoder.inverse_transform(y_test)\n",
    "texts = test_data[tokenized_field]\n",
    "raw_texts = test_data[content_field]\n",
    "titles = test_data[\"Title\"]\n",
    "\n",
    "for pred, act, txt, raw_txt, title in zip(l_pred, l_true, texts, raw_texts, titles):\n",
    "    print(f\"TRUE: {act}\\nPREDICTION: {pred}\\n\")\n",
    "    print(f\"{title}\\n----------\")\n",
    "    print(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "\n",
      "normalize_embeddings = True, learning_rate = 1, vocab_size = None, epochs=30\n",
      "Parameter Settings:\n",
      " Sample size = -1, Max. number of words per question = 100, Number of Top Labels used = 100\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 100)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          365568      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          51300       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 782,436\n",
      "Trainable params: 782,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dschr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\dschr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics with optimized threshold of 0.43\n",
      " Macro Evaluation: f1_Score= 0.5090096015650044 , Recall = 0.44893302869566176 , Precision = 0.6111371903996836\n",
      " Micro Evaluation: f1_Score= 0.5972185778011021 , Recall = 0.5130362912314975 , Precision = 0.7144501412577169\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([X_test_t_padded, X_test_b_padded], batch_size=64)\n",
    "output_evaluation(model, sample_size, max_question_words, n_top_labels, y_test, predictions, normalize_embeddings, 1, None, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19526, 100)\n",
      "(19526, 100)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline using Title and Body Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from toolbox.data_prep_helpers import *\n",
    "from toolbox.evaluation import *\n",
    "\n",
    "#from models.lstm_classifier import create_model\n",
    "from models.title_body_lstm import create_model as tb_create_model\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pythonquestions/\"\n",
    "ft_path = \"sg_model.ft\"  # set this to None if you want to train your own fasttext embeddings\n",
    "n_top_labels = 100\n",
    "n_epochs = 300\n",
    "max_question_words = 100\n",
    "sample_size = -1  # set to -1 to use entire data\n",
    "normalize_embeddings = True\n",
    "use_titles = False\n",
    "\n",
    "tokenized_field = \"q_all_body_tokenized\"\n",
    "content_field = \"Body_q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dschr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from cached pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(607282, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(data_path, ignore_cache=False, tokenized_field=tokenized_field, content_field=content_field)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body_q</th>\n",
       "      <th>tags</th>\n",
       "      <th>q_all_body_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>How can I find the full path to a font from it...</td>\n",
       "      <td>I am using the Photoshop's javascript API to f...</td>\n",
       "      <td>[python, osx, fonts, photoshop]</td>\n",
       "      <td>[i, am, using, the, photoshop, 's, javascript,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>502</td>\n",
       "      <td>Get a preview JPEG of a PDF on Windows?</td>\n",
       "      <td>I have a cross-platform (Python) application w...</td>\n",
       "      <td>[python, windows, image, pdf]</td>\n",
       "      <td>[i, have, a, cross-platform, (, python, ), app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>535</td>\n",
       "      <td>Continuous Integration System for a Python Cod...</td>\n",
       "      <td>I'm starting work on a hobby project with a py...</td>\n",
       "      <td>[python, continuous-integration, extreme-progr...</td>\n",
       "      <td>[i, 'm, starting, work, on, a, hobby, project,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>594</td>\n",
       "      <td>cx_Oracle: How do I iterate over a result set?</td>\n",
       "      <td>There are several ways to iterate over a resul...</td>\n",
       "      <td>[python, sql, database, oracle, cx-oracle]</td>\n",
       "      <td>[there, are, several, ways, to, iterate, over,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>683</td>\n",
       "      <td>Using 'in' to match an attribute of Python obj...</td>\n",
       "      <td>I don't remember whether I was dreaming or not...</td>\n",
       "      <td>[python, arrays, iteration]</td>\n",
       "      <td>[i, do, n't, remember, whether, i, was, dreami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Title  \\\n",
       "0  469  How can I find the full path to a font from it...   \n",
       "1  502            Get a preview JPEG of a PDF on Windows?   \n",
       "2  535  Continuous Integration System for a Python Cod...   \n",
       "3  594     cx_Oracle: How do I iterate over a result set?   \n",
       "4  683  Using 'in' to match an attribute of Python obj...   \n",
       "\n",
       "                                              Body_q  \\\n",
       "0  I am using the Photoshop's javascript API to f...   \n",
       "1  I have a cross-platform (Python) application w...   \n",
       "2  I'm starting work on a hobby project with a py...   \n",
       "3  There are several ways to iterate over a resul...   \n",
       "4  I don't remember whether I was dreaming or not...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                    [python, osx, fonts, photoshop]   \n",
       "1                      [python, windows, image, pdf]   \n",
       "2  [python, continuous-integration, extreme-progr...   \n",
       "3         [python, sql, database, oracle, cx-oracle]   \n",
       "4                        [python, arrays, iteration]   \n",
       "\n",
       "                                q_all_body_tokenized  \n",
       "0  [i, am, using, the, photoshop, 's, javascript,...  \n",
       "1  [i, have, a, cross-platform, (, python, ), app...  \n",
       "2  [i, 'm, starting, work, on, a, hobby, project,...  \n",
       "3  [there, are, several, ways, to, iterate, over,...  \n",
       "4  [i, do, n't, remember, whether, i, was, dreami...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df.sample(sample_size) if sample_size > 0 else df\n",
    "del df\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_html_tags(chunk, [\"Body_q\"])\n",
    "#print(f\"{i}: generating question level tokens\")\n",
    "sample[\"q_title_tokenized\"] = sample[\"Title\"].apply(generate_question_level_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607282, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(607282, 6)\n",
      "(606841, 6)\n",
      "deleting element python from top_tags\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(425658, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have some nans in our tags which break target encoding\n",
    "print(sample.shape)\n",
    "sample = sample[sample[\"tags\"].apply(lambda tags: all([isinstance(t, str) for t in tags]))]\n",
    "print(sample.shape)\n",
    "\n",
    "\n",
    "# Reduce the number of tags and adjust dataframe accordingly\n",
    "sample = reduce_number_of_tags(sample, n_top_labels)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [osx]\n",
       "1    [windows, image]\n",
       "3     [sql, database]\n",
       "4            [arrays]\n",
       "5       [django, oop]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"tags\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Questions that have more than \"max_question_words\" words and questions with no words\n",
    "data = sample[sample[tokenized_field].apply(len) <= max_question_words]\n",
    "data = data[data[\"q_all_body_tokenized\"].apply(len) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97627, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97627, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "idx = np.random.randint(0,len(data),100)\n",
    "validation_questions = data.iloc[idx]\n",
    "data = data.drop(validation_questions.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97527, 6)\n",
      "(100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(validation_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or create FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ft_path is not None:\n",
    "    wv = load_fasttext_embeddings(ft_path)\n",
    "else:\n",
    "    wv = create_FastText_embeddings(df, content_field)   \n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97527, 40, 100)\n",
      "(97527, 100, 100)\n",
      "(97527, 100)\n"
     ]
    }
   ],
   "source": [
    "# Apply word embeddings, insert paddings and encode the labels with an MultiLabelBinarizer\n",
    "\n",
    "X_t = data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_b = data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "padding_element = np.array([0.0] * X_t.iloc[0].shape[-1])\n",
    "\n",
    "X_t = pad_sequences(X_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_b = pad_sequences(X_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_t.shape)\n",
    "print(X_b.shape)\n",
    "\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "label_encoder.fit(data[\"tags\"])\n",
    "y = label_encoder.transform(data[\"tags\"])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Apply grid search to find best hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_layer_size': 256, 'lstm_dropout': 0.0, 'num_mid_dense': 1, 'output_dim': 100}\n",
      "Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 2:55:13 - loss: 0.6937 - accuracy: 0.4856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:27 - loss: 0.6848 - accuracy: 0.6695 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:45 - loss: 0.6737 - accuracy: 0.7305 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 27:53 - loss: 0.6564 - accuracy: 0.7659 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:16 - loss: 0.6281 - accuracy: 0.7881 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:07 - loss: 0.5707 - accuracy: 0.8048 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 26:31 - loss: 0.4882 - accuracy: 0.8225 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 26:23 - loss: 0.4071 - accuracy: 0.8328 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 26:12 - loss: 0.3469 - accuracy: 0.8513 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 26:17 - loss: 0.2905 - accuracy: 0.8798 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 26:45 - loss: 0.2446 - accuracy: 0.9180 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 27:15 - loss: 0.2001 - accuracy: 0.9499 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 27:24 - loss: 0.1653 - accuracy: 0.9630 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 28:40 - loss: 0.1392 - accuracy: 0.9677 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 28:18 - loss: 0.1204 - accuracy: 0.9723 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 28:22 - loss: 0.1079 - accuracy: 0.9763 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 28:44 - loss: 0.0877 - accuracy: 0.9876 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 28:11 - loss: 0.0959 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 28:07 - loss: 0.0870 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 27:25 - loss: 0.0862 - accuracy: 0.9851 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 28:16 - loss: 0.0843 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 28:20 - loss: 0.0793 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 28:22 - loss: 0.0919 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 29:20 - loss: 0.0934 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 25/150\n",
      "  128/78101 [..............................] - ETA: 28:53 - loss: 0.0798 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 26/150\n",
      "  128/78101 [..............................] - ETA: 28:49 - loss: 0.0854 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 27/150\n",
      "  128/78101 [..............................] - ETA: 28:49 - loss: 0.0764 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 28/150\n",
      "  128/78101 [..............................] - ETA: 28:40 - loss: 0.0727 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 29/150\n",
      "  128/78101 [..............................] - ETA: 28:01 - loss: 0.0792 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 30/150\n",
      "  128/78101 [..............................] - ETA: 28:10 - loss: 0.0800 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 31/150\n",
      "  128/78101 [..............................] - ETA: 28:12 - loss: 0.0825 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 32/150\n",
      "  128/78101 [..............................] - ETA: 28:31 - loss: 0.0764 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 33/150\n",
      "  128/78101 [..............................] - ETA: 29:33 - loss: 0.0785 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 34/150\n",
      "  128/78101 [..............................] - ETA: 28:46 - loss: 0.0767 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 35/150\n",
      "  128/78101 [..............................] - ETA: 29:40 - loss: 0.0712 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 36/150\n",
      "  128/78101 [..............................] - ETA: 28:06 - loss: 0.0707 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 37/150\n",
      "  128/78101 [..............................] - ETA: 29:29 - loss: 0.0712 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 38/150\n",
      "  128/78101 [..............................] - ETA: 28:55 - loss: 0.0708 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 39/150\n",
      "  128/78101 [..............................] - ETA: 28:59 - loss: 0.0726 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 40/150\n",
      "  128/78101 [..............................] - ETA: 30:03 - loss: 0.0704 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 41/150\n",
      "  128/78101 [..............................] - ETA: 28:37 - loss: 0.0655 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 42/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.0651 - accuracy: 0.9873 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 43/150\n",
      "  128/78101 [..............................] - ETA: 29:35 - loss: 0.0662 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 44/150\n",
      "  128/78101 [..............................] - ETA: 29:04 - loss: 0.0708 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 45/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0693 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 46/150\n",
      "  128/78101 [..............................] - ETA: 29:09 - loss: 0.0745 - accuracy: 0.9854 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 47/150\n",
      "  128/78101 [..............................] - ETA: 29:29 - loss: 0.0707 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 48/150\n",
      "  128/78101 [..............................] - ETA: 29:21 - loss: 0.0643 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 49/150\n",
      "  128/78101 [..............................] - ETA: 29:24 - loss: 0.0686 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 50/150\n",
      "  128/78101 [..............................] - ETA: 29:13 - loss: 0.0651 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 51/150\n",
      "  128/78101 [..............................] - ETA: 29:36 - loss: 0.0675 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 52/150\n",
      "  128/78101 [..............................] - ETA: 30:55 - loss: 0.0639 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 53/150\n",
      "  128/78101 [..............................] - ETA: 31:22 - loss: 0.0694 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 54/150\n",
      "  128/78101 [..............................] - ETA: 31:20 - loss: 0.0663 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 55/150\n",
      "  128/78101 [..............................] - ETA: 31:14 - loss: 0.0659 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 56/150\n",
      "  128/78101 [..............................] - ETA: 30:24 - loss: 0.0680 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 57/150\n",
      "  128/78101 [..............................] - ETA: 29:31 - loss: 0.0666 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 58/150\n",
      "  128/78101 [..............................] - ETA: 29:26 - loss: 0.0653 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 59/150\n",
      "  128/78101 [..............................] - ETA: 30:44 - loss: 0.0646 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 60/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.0656 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 61/150\n",
      "  128/78101 [..............................] - ETA: 30:45 - loss: 0.0689 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 62/150\n",
      "  128/78101 [..............................] - ETA: 30:09 - loss: 0.0638 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 63/150\n",
      "  128/78101 [..............................] - ETA: 32:30 - loss: 0.0635 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 64/150\n",
      "  128/78101 [..............................] - ETA: 30:32 - loss: 0.0697 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 65/150\n",
      "  128/78101 [..............................] - ETA: 30:21 - loss: 0.0669 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 66/150\n",
      "  128/78101 [..............................] - ETA: 30:10 - loss: 0.0682 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 67/150\n",
      "  128/78101 [..............................] - ETA: 30:01 - loss: 0.0655 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 68/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0655 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 69/150\n",
      "  128/78101 [..............................] - ETA: 29:59 - loss: 0.0619 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 70/150\n",
      "  128/78101 [..............................] - ETA: 30:17 - loss: 0.0676 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 71/150\n",
      "  128/78101 [..............................] - ETA: 29:53 - loss: 0.0634 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 72/150\n",
      "  128/78101 [..............................] - ETA: 30:12 - loss: 0.0703 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 73/150\n",
      "  128/78101 [..............................] - ETA: 30:22 - loss: 0.0650 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 74/150\n",
      "  128/78101 [..............................] - ETA: 32:17 - loss: 0.0643 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 75/150\n",
      "  128/78101 [..............................] - ETA: 30:41 - loss: 0.0648 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 76/150\n",
      "  128/78101 [..............................] - ETA: 30:10 - loss: 0.0659 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 77/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0710 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 78/150\n",
      "  128/78101 [..............................] - ETA: 29:59 - loss: 0.0686 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 79/150\n",
      "  128/78101 [..............................] - ETA: 30:32 - loss: 0.0698 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 80/150\n",
      "  128/78101 [..............................] - ETA: 31:50 - loss: 0.0691 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 81/150\n",
      "  128/78101 [..............................] - ETA: 33:00 - loss: 0.0692 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 82/150\n",
      "  128/78101 [..............................] - ETA: 30:57 - loss: 0.0653 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 83/150\n",
      "  128/78101 [..............................] - ETA: 32:16 - loss: 0.0670 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 84/150\n",
      "  128/78101 [..............................] - ETA: 34:13 - loss: 0.0679 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 85/150\n",
      "  128/78101 [..............................] - ETA: 30:40 - loss: 0.0650 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 86/150\n",
      "  128/78101 [..............................] - ETA: 31:06 - loss: 0.0674 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 87/150\n",
      "  128/78101 [..............................] - ETA: 30:43 - loss: 0.0640 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 88/150\n",
      "  128/78101 [..............................] - ETA: 30:43 - loss: 0.0650 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 89/150\n",
      "  128/78101 [..............................] - ETA: 30:42 - loss: 0.0622 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 90/150\n",
      "  128/78101 [..............................] - ETA: 30:36 - loss: 0.0690 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 91/150\n",
      "  128/78101 [..............................] - ETA: 30:53 - loss: 0.0686 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 92/150\n",
      "  128/78101 [..............................] - ETA: 31:39 - loss: 0.0642 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 93/150\n",
      "  128/78101 [..............................] - ETA: 31:54 - loss: 0.0658 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 94/150\n",
      "  128/78101 [..............................] - ETA: 31:55 - loss: 0.0637 - accuracy: 0.9873 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 95/150\n",
      "  128/78101 [..............................] - ETA: 31:42 - loss: 0.0643 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 96/150\n",
      "  128/78101 [..............................] - ETA: 32:07 - loss: 0.0598 - accuracy: 0.9879 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 97/150\n",
      "  128/78101 [..............................] - ETA: 31:33 - loss: 0.0686 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 98/150\n",
      "  128/78101 [..............................] - ETA: 30:02 - loss: 0.0694 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 3:22:28 - loss: 0.6940 - accuracy: 0.5190 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 27:23 - loss: 0.6854 - accuracy: 0.6717 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:49 - loss: 0.6751 - accuracy: 0.7272 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:15 - loss: 0.6587 - accuracy: 0.7533 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:42 - loss: 0.6321 - accuracy: 0.7677 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:48 - loss: 0.5771 - accuracy: 0.7856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 28:28 - loss: 0.4874 - accuracy: 0.8058 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 28:02 - loss: 0.4053 - accuracy: 0.8394 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 29:48 - loss: 0.3362 - accuracy: 0.8685 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 29:39 - loss: 0.2840 - accuracy: 0.9289 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 29:01 - loss: 0.2324 - accuracy: 0.9447 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.1863 - accuracy: 0.9596 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 29:18 - loss: 0.1537 - accuracy: 0.9770 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 30:27 - loss: 0.1281 - accuracy: 0.9785 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 30:11 - loss: 0.1126 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 30:34 - loss: 0.0944 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 30:20 - loss: 0.0934 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 30:08 - loss: 0.0949 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 30:23 - loss: 0.0887 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0916 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 30:19 - loss: 0.0885 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 30:03 - loss: 0.0861 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0784 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 30:25 - loss: 0.0813 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 25/150\n",
      "  128/78101 [..............................] - ETA: 32:26 - loss: 0.0832 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 26/150\n",
      "  128/78101 [..............................] - ETA: 32:39 - loss: 0.0755 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 27/150\n",
      "  128/78101 [..............................] - ETA: 30:33 - loss: 0.0769 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 28/150\n",
      "  128/78101 [..............................] - ETA: 30:51 - loss: 0.0760 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 29/150\n",
      "  128/78101 [..............................] - ETA: 32:01 - loss: 0.0739 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 30/150\n",
      "  128/78101 [..............................] - ETA: 31:12 - loss: 0.0799 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 31/150\n",
      "  128/78101 [..............................] - ETA: 33:49 - loss: 0.0757 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 32/150\n",
      "  128/78101 [..............................] - ETA: 32:01 - loss: 0.0795 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 33/150\n",
      "  128/78101 [..............................] - ETA: 31:14 - loss: 0.0723 - accuracy: 0.9873 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 34/150\n",
      "  128/78101 [..............................] - ETA: 31:00 - loss: 0.0736 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 35/150\n",
      "  128/78101 [..............................] - ETA: 31:16 - loss: 0.0739 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 36/150\n",
      "  128/78101 [..............................] - ETA: 31:07 - loss: 0.0667 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 37/150\n",
      "  128/78101 [..............................] - ETA: 31:25 - loss: 0.0746 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 38/150\n",
      "  128/78101 [..............................] - ETA: 31:21 - loss: 0.0653 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 39/150\n",
      "  128/78101 [..............................] - ETA: 31:18 - loss: 0.0682 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 40/150\n",
      "  128/78101 [..............................] - ETA: 31:16 - loss: 0.0615 - accuracy: 0.9877 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 41/150\n",
      "  128/78101 [..............................] - ETA: 31:07 - loss: 0.0711 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 42/150\n",
      "  128/78101 [..............................] - ETA: 31:16 - loss: 0.0672 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 43/150\n",
      "  128/78101 [..............................] - ETA: 30:55 - loss: 0.0678 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 44/150\n",
      "  128/78101 [..............................] - ETA: 31:17 - loss: 0.0659 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 45/150\n",
      "  128/78101 [..............................] - ETA: 31:13 - loss: 0.0672 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 46/150\n",
      "  128/78101 [..............................] - ETA: 30:57 - loss: 0.0680 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 47/150\n",
      "  128/78101 [..............................] - ETA: 31:20 - loss: 0.0697 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 48/150\n",
      "  128/78101 [..............................] - ETA: 31:04 - loss: 0.0672 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 49/150\n",
      "  128/78101 [..............................] - ETA: 31:13 - loss: 0.0665 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 50/150\n",
      "  128/78101 [..............................] - ETA: 30:55 - loss: 0.0637 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 51/150\n",
      "  128/78101 [..............................] - ETA: 31:17 - loss: 0.0713 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 52/150\n",
      "  128/78101 [..............................] - ETA: 31:10 - loss: 0.0713 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 53/150\n",
      "  128/78101 [..............................] - ETA: 31:04 - loss: 0.0710 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 54/150\n",
      "  128/78101 [..............................] - ETA: 31:09 - loss: 0.0685 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 55/150\n",
      "  128/78101 [..............................] - ETA: 30:52 - loss: 0.0683 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 56/150\n",
      "  128/78101 [..............................] - ETA: 31:23 - loss: 0.0696 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 57/150\n",
      "  128/78101 [..............................] - ETA: 30:57 - loss: 0.0696 - accuracy: 0.9854 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 58/150\n",
      "  128/78101 [..............................] - ETA: 31:18 - loss: 0.0671 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 59/150\n",
      "  128/78101 [..............................] - ETA: 31:15 - loss: 0.0631 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 60/150\n",
      "  128/78101 [..............................] - ETA: 31:20 - loss: 0.0738 - accuracy: 0.9851 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 61/150\n",
      "  128/78101 [..............................] - ETA: 31:14 - loss: 0.0697 - accuracy: 0.9854 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 62/150\n",
      "  128/78101 [..............................] - ETA: 30:58 - loss: 0.0686 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 63/150\n",
      "  128/78101 [..............................] - ETA: 31:09 - loss: 0.0659 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 64/150\n",
      "  128/78101 [..............................] - ETA: 30:54 - loss: 0.0659 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 65/150\n",
      "  128/78101 [..............................] - ETA: 31:15 - loss: 0.0689 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 66/150\n",
      "  128/78101 [..............................] - ETA: 31:45 - loss: 0.0673 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 67/150\n",
      "  128/78101 [..............................] - ETA: 33:12 - loss: 0.0691 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 68/150\n",
      "  128/78101 [..............................] - ETA: 36:53 - loss: 0.0716 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 69/150\n",
      "  128/78101 [..............................] - ETA: 35:04 - loss: 0.0642 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 70/150\n",
      "  128/78101 [..............................] - ETA: 31:35 - loss: 0.0677 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 71/150\n",
      "  128/78101 [..............................] - ETA: 31:17 - loss: 0.0661 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 72/150\n",
      "  128/78101 [..............................] - ETA: 31:46 - loss: 0.0647 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 73/150\n",
      "  128/78101 [..............................] - ETA: 31:25 - loss: 0.0647 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 74/150\n",
      "  128/78101 [..............................] - ETA: 31:20 - loss: 0.0651 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 75/150\n",
      "  128/78101 [..............................] - ETA: 31:36 - loss: 0.0681 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 76/150\n",
      "  128/78101 [..............................] - ETA: 44:27 - loss: 0.0714 - accuracy: 0.9849 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 77/150\n",
      "  128/78101 [..............................] - ETA: 32:03 - loss: 0.0639 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 78/150\n",
      "  128/78101 [..............................] - ETA: 30:20 - loss: 0.0642 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 79/150\n",
      "  128/78101 [..............................] - ETA: 30:59 - loss: 0.0645 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 80/150\n",
      "  128/78101 [..............................] - ETA: 34:57 - loss: 0.0657 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 81/150\n",
      "  128/78101 [..............................] - ETA: 34:36 - loss: 0.0662 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 82/150\n",
      "  128/78101 [..............................] - ETA: 35:04 - loss: 0.0683 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 83/150\n",
      "  128/78101 [..............................] - ETA: 34:46 - loss: 0.0647 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 84/150\n",
      "  128/78101 [..............................] - ETA: 34:51 - loss: 0.0677 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 85/150\n",
      "  128/78101 [..............................] - ETA: 34:51 - loss: 0.0627 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 86/150\n",
      "  128/78101 [..............................] - ETA: 34:32 - loss: 0.0650 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 87/150\n",
      "  128/78101 [..............................] - ETA: 34:11 - loss: 0.0682 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 88/150\n",
      "  128/78101 [..............................] - ETA: 34:01 - loss: 0.0658 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 89/150\n",
      "  128/78101 [..............................] - ETA: 34:02 - loss: 0.0670 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 90/150\n",
      "  128/78101 [..............................] - ETA: 32:47 - loss: 0.0673 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00average min val_loss: 0.06622259205196354 -- epochs: [98, 90] -- time: 1027.04 seconds\n",
      "{'lstm_layer_size': 256, 'lstm_dropout': 0.0, 'num_mid_dense': 0, 'output_dim': 100}\n",
      "Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 3:17:12 - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:25 - loss: 0.6757 - accuracy: 0.8187 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:22 - loss: 0.6561 - accuracy: 0.9199 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:30 - loss: 0.6233 - accuracy: 0.9537 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:11 - loss: 0.5576 - accuracy: 0.9650 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:25 - loss: 0.4359 - accuracy: 0.9718 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 26:59 - loss: 0.3306 - accuracy: 0.9768 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 27:23 - loss: 0.2472 - accuracy: 0.9816 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 27:38 - loss: 0.1895 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 28:26 - loss: 0.1393 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 28:54 - loss: 0.1106 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 28:40 - loss: 0.0876 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 29:07 - loss: 0.0795 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 28:29 - loss: 0.0757 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0799 - accuracy: 0.9853 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 29:39 - loss: 0.0772 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 29:45 - loss: 0.0785 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 29:29 - loss: 0.0789 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 29:37 - loss: 0.0761 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 30:49 - loss: 0.0798 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 31:47 - loss: 0.0865 - accuracy: 0.9851 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 30:09 - loss: 0.0828 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 29:30 - loss: 0.0799 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 30:45 - loss: 0.0760 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 69:43:39 - loss: 0.6947 - accuracy: 0.4802 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:39 - loss: 0.6787 - accuracy: 0.7405 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:12 - loss: 0.6612 - accuracy: 0.8834 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:16 - loss: 0.6342 - accuracy: 0.9520 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 25:31 - loss: 0.5913 - accuracy: 0.9661 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:07 - loss: 0.4979 - accuracy: 0.9750 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 26:30 - loss: 0.3785 - accuracy: 0.9827 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 26:24 - loss: 0.2801 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 27:05 - loss: 0.2133 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 26:35 - loss: 0.1536 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 27:57 - loss: 0.1209 - accuracy: 0.9854 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 28:48 - loss: 0.0957 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 30:08 - loss: 0.0811 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 30:02 - loss: 0.0771 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 28:54 - loss: 0.0769 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 28:41 - loss: 0.0805 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 28:12 - loss: 0.0711 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 28:21 - loss: 0.0755 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 28:09 - loss: 0.0758 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 28:47 - loss: 0.0740 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 29:15 - loss: 0.0807 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 29:04 - loss: 0.0795 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 28:00 - loss: 0.0863 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 29:15 - loss: 0.0824 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00average min val_loss: 0.07582464436620312 -- epochs: [24, 24] -- time: 687.41 seconds\n",
      "{'lstm_layer_size': 256, 'lstm_dropout': 0.2, 'num_mid_dense': 1, 'output_dim': 100}\n",
      "Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 3:39:43 - loss: 0.6933 - accuracy: 0.4865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:27 - loss: 0.6818 - accuracy: 0.7081 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:52 - loss: 0.6672 - accuracy: 0.7893 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:41 - loss: 0.6451 - accuracy: 0.8172 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:48 - loss: 0.6082 - accuracy: 0.8267 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:46 - loss: 0.5364 - accuracy: 0.8342 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 26:27 - loss: 0.4435 - accuracy: 0.8516 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 26:51 - loss: 0.3723 - accuracy: 0.8638 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 26:37 - loss: 0.3176 - accuracy: 0.8869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 27:03 - loss: 0.2687 - accuracy: 0.9161 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 27:31 - loss: 0.2312 - accuracy: 0.9434 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 27:13 - loss: 0.1946 - accuracy: 0.9527 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 27:57 - loss: 0.1690 - accuracy: 0.9663 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 27:34 - loss: 0.1410 - accuracy: 0.9666 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 27:41 - loss: 0.1227 - accuracy: 0.9763 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 28:54 - loss: 0.0962 - accuracy: 0.9777 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 27:57 - loss: 0.0894 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 28:46 - loss: 0.0850 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 29:18 - loss: 0.0862 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 28:33 - loss: 0.0821 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 28:30 - loss: 0.0825 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 29:12 - loss: 0.0816 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 29:15 - loss: 0.0832 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 29:11 - loss: 0.0789 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 25/150\n",
      "  128/78101 [..............................] - ETA: 29:19 - loss: 0.0826 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 26/150\n",
      "  128/78101 [..............................] - ETA: 29:20 - loss: 0.0804 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 27/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0749 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 28/150\n",
      "  128/78101 [..............................] - ETA: 28:52 - loss: 0.0772 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 29/150\n",
      "  128/78101 [..............................] - ETA: 30:27 - loss: 0.0717 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 30/150\n",
      "  128/78101 [..............................] - ETA: 29:22 - loss: 0.0758 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 31/150\n",
      "  128/78101 [..............................] - ETA: 29:48 - loss: 0.0771 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 32/150\n",
      "  128/78101 [..............................] - ETA: 30:50 - loss: 0.0773 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 33/150\n",
      "  128/78101 [..............................] - ETA: 29:13 - loss: 0.0725 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 34/150\n",
      "  128/78101 [..............................] - ETA: 30:25 - loss: 0.0780 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 35/150\n",
      "  128/78101 [..............................] - ETA: 29:16 - loss: 0.0792 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 36/150\n",
      "  128/78101 [..............................] - ETA: 29:24 - loss: 0.0690 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 37/150\n",
      "  128/78101 [..............................] - ETA: 29:02 - loss: 0.0699 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 38/150\n",
      "  128/78101 [..............................] - ETA: 30:53 - loss: 0.0718 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 39/150\n",
      "  128/78101 [..............................] - ETA: 30:25 - loss: 0.0753 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 40/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.0666 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 41/150\n",
      "  128/78101 [..............................] - ETA: 30:02 - loss: 0.0705 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 42/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0654 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 43/150\n",
      "  128/78101 [..............................] - ETA: 30:20 - loss: 0.0673 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 44/150\n",
      "  128/78101 [..............................] - ETA: 29:53 - loss: 0.0676 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 45/150\n",
      "  128/78101 [..............................] - ETA: 30:20 - loss: 0.0675 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 46/150\n",
      "  128/78101 [..............................] - ETA: 30:18 - loss: 0.0684 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 47/150\n",
      "  128/78101 [..............................] - ETA: 29:56 - loss: 0.0693 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 48/150\n",
      "  128/78101 [..............................] - ETA: 30:23 - loss: 0.0702 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 49/150\n",
      "  128/78101 [..............................] - ETA: 29:58 - loss: 0.0680 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 50/150\n",
      "  128/78101 [..............................] - ETA: 30:13 - loss: 0.0666 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 51/150\n",
      "  128/78101 [..............................] - ETA: 30:01 - loss: 0.0676 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 52/150\n",
      "  128/78101 [..............................] - ETA: 30:11 - loss: 0.0645 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 53/150\n",
      "  128/78101 [..............................] - ETA: 30:10 - loss: 0.0666 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 54/150\n",
      "  128/78101 [..............................] - ETA: 29:57 - loss: 0.0624 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 55/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0753 - accuracy: 0.9846 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 56/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0721 - accuracy: 0.9854 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 57/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0697 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 58/150\n",
      "  128/78101 [..............................] - ETA: 30:06 - loss: 0.0665 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 59/150\n",
      "  128/78101 [..............................] - ETA: 30:12 - loss: 0.0670 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 60/150\n",
      "  128/78101 [..............................] - ETA: 30:22 - loss: 0.0656 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 61/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.0640 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 62/150\n",
      "  128/78101 [..............................] - ETA: 29:54 - loss: 0.0648 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 63/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.0671 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 64/150\n",
      "  128/78101 [..............................] - ETA: 30:03 - loss: 0.0670 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 65/150\n",
      "  128/78101 [..............................] - ETA: 30:00 - loss: 0.0683 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 66/150\n",
      "  128/78101 [..............................] - ETA: 30:18 - loss: 0.0653 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 67/150\n",
      "  128/78101 [..............................] - ETA: 30:35 - loss: 0.0702 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 68/150\n",
      "  128/78101 [..............................] - ETA: 32:01 - loss: 0.0658 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 69/150\n",
      "  128/78101 [..............................] - ETA: 33:34 - loss: 0.0647 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 70/150\n",
      "  128/78101 [..............................] - ETA: 31:07 - loss: 0.0683 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 71/150\n",
      "  128/78101 [..............................] - ETA: 30:33 - loss: 0.0700 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 72/150\n",
      "  128/78101 [..............................] - ETA: 30:17 - loss: 0.0651 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 73/150\n",
      "  128/78101 [..............................] - ETA: 30:34 - loss: 0.0700 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 74/150\n",
      "  128/78101 [..............................] - ETA: 30:29 - loss: 0.0659 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 75/150\n",
      "  128/78101 [..............................] - ETA: 30:11 - loss: 0.0659 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 76/150\n",
      "  128/78101 [..............................] - ETA: 30:24 - loss: 0.0637 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 77/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0658 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 78/150\n",
      "  128/78101 [..............................] - ETA: 30:21 - loss: 0.0661 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 79/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0722 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 80/150\n",
      "  128/78101 [..............................] - ETA: 30:38 - loss: 0.0644 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 81/150\n",
      "  128/78101 [..............................] - ETA: 30:29 - loss: 0.0687 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 82/150\n",
      "  128/78101 [..............................] - ETA: 30:12 - loss: 0.0652 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 83/150\n",
      "  128/78101 [..............................] - ETA: 30:36 - loss: 0.0667 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 84/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0701 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 85/150\n",
      "  128/78101 [..............................] - ETA: 30:33 - loss: 0.0650 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 86/150\n",
      "  128/78101 [..............................] - ETA: 30:11 - loss: 0.0667 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 87/150\n",
      "  128/78101 [..............................] - ETA: 30:37 - loss: 0.0650 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 88/150\n",
      "  128/78101 [..............................] - ETA: 30:30 - loss: 0.0679 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 89/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0695 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 90/150\n",
      "  128/78101 [..............................] - ETA: 30:30 - loss: 0.0638 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 91/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0689 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 92/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0680 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 93/150\n",
      "  128/78101 [..............................] - ETA: 30:25 - loss: 0.0668 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 94/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0671 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 95/150\n",
      "  128/78101 [..............................] - ETA: 32:17 - loss: 0.0638 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 96/150\n",
      "  128/78101 [..............................] - ETA: 29:56 - loss: 0.0662 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 3:37:17 - loss: 0.6929 - accuracy: 0.5249 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:12 - loss: 0.6837 - accuracy: 0.7160 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:37 - loss: 0.6722 - accuracy: 0.7866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:29 - loss: 0.6529 - accuracy: 0.8283 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:54 - loss: 0.6194 - accuracy: 0.8352 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 26:25 - loss: 0.5509 - accuracy: 0.8409 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 26:08 - loss: 0.4547 - accuracy: 0.8552 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 26:43 - loss: 0.3722 - accuracy: 0.8896 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 26:19 - loss: 0.3019 - accuracy: 0.9101 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 27:13 - loss: 0.2391 - accuracy: 0.9395 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 27:33 - loss: 0.1855 - accuracy: 0.9478 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 27:39 - loss: 0.1484 - accuracy: 0.9666 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 28:35 - loss: 0.1144 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 28:00 - loss: 0.0952 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 28:47 - loss: 0.0861 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 28:54 - loss: 0.0875 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 29:21 - loss: 0.0836 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 29:35 - loss: 0.0843 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 29:30 - loss: 0.0910 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 29:30 - loss: 0.0824 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 29:20 - loss: 0.0843 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 29:40 - loss: 0.0859 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 29:23 - loss: 0.0853 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 29:41 - loss: 0.0831 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 25/150\n",
      "  128/78101 [..............................] - ETA: 29:19 - loss: 0.0890 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 26/150\n",
      "  128/78101 [..............................] - ETA: 31:16 - loss: 0.0766 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 27/150\n",
      "  128/78101 [..............................] - ETA: 30:31 - loss: 0.0776 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 28/150\n",
      "  128/78101 [..............................] - ETA: 29:56 - loss: 0.0779 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 29/150\n",
      "  128/78101 [..............................] - ETA: 30:56 - loss: 0.0822 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 30/150\n",
      "  128/78101 [..............................] - ETA: 31:06 - loss: 0.0709 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 31/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0759 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 32/150\n",
      "  128/78101 [..............................] - ETA: 31:42 - loss: 0.0688 - accuracy: 0.9871 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 33/150\n",
      "  128/78101 [..............................] - ETA: 31:19 - loss: 0.0750 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 34/150\n",
      "  128/78101 [..............................] - ETA: 30:19 - loss: 0.0716 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 35/150\n",
      "  128/78101 [..............................] - ETA: 29:54 - loss: 0.0710 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 36/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0710 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 37/150\n",
      "  128/78101 [..............................] - ETA: 29:54 - loss: 0.0738 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 38/150\n",
      "  128/78101 [..............................] - ETA: 30:19 - loss: 0.0702 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 39/150\n",
      "  128/78101 [..............................] - ETA: 29:59 - loss: 0.0730 - accuracy: 0.9851 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 40/150\n",
      "  128/78101 [..............................] - ETA: 30:10 - loss: 0.0670 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 41/150\n",
      "  128/78101 [..............................] - ETA: 30:15 - loss: 0.0683 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 42/150\n",
      "  128/78101 [..............................] - ETA: 31:45 - loss: 0.0656 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 43/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0649 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 44/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0690 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 45/150\n",
      "  128/78101 [..............................] - ETA: 30:20 - loss: 0.0717 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 46/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0712 - accuracy: 0.9853 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 47/150\n",
      "  128/78101 [..............................] - ETA: 30:14 - loss: 0.0676 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 48/150\n",
      "  128/78101 [..............................] - ETA: 31:54 - loss: 0.0647 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 49/150\n",
      "  128/78101 [..............................] - ETA: 31:53 - loss: 0.0664 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 50/150\n",
      "  128/78101 [..............................] - ETA: 30:27 - loss: 0.0681 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 51/150\n",
      "  128/78101 [..............................] - ETA: 30:24 - loss: 0.0644 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 52/150\n",
      "  128/78101 [..............................] - ETA: 30:27 - loss: 0.0670 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 53/150\n",
      "  128/78101 [..............................] - ETA: 30:23 - loss: 0.0709 - accuracy: 0.9855 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 54/150\n",
      "  128/78101 [..............................] - ETA: 30:26 - loss: 0.0671 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 55/150\n",
      "  128/78101 [..............................] - ETA: 32:39 - loss: 0.0647 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 56/150\n",
      "  128/78101 [..............................] - ETA: 30:26 - loss: 0.0654 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 57/150\n",
      "  128/78101 [..............................] - ETA: 30:42 - loss: 0.0669 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 58/150\n",
      "  128/78101 [..............................] - ETA: 30:34 - loss: 0.0637 - accuracy: 0.9868 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 59/150\n",
      "  128/78101 [..............................] - ETA: 30:53 - loss: 0.0641 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 60/150\n",
      "  128/78101 [..............................] - ETA: 30:31 - loss: 0.0647 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 61/150\n",
      "  128/78101 [..............................] - ETA: 30:44 - loss: 0.0647 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 62/150\n",
      "  128/78101 [..............................] - ETA: 30:45 - loss: 0.0667 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 63/150\n",
      "  128/78101 [..............................] - ETA: 30:37 - loss: 0.0667 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 64/150\n",
      "  128/78101 [..............................] - ETA: 30:42 - loss: 0.0669 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 65/150\n",
      "  128/78101 [..............................] - ETA: 30:36 - loss: 0.0705 - accuracy: 0.9856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 66/150\n",
      "  128/78101 [..............................] - ETA: 30:39 - loss: 0.0644 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 67/150\n",
      "  128/78101 [..............................] - ETA: 30:40 - loss: 0.0686 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 68/150\n",
      "  128/78101 [..............................] - ETA: 31:04 - loss: 0.0704 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 69/150\n",
      "  128/78101 [..............................] - ETA: 31:28 - loss: 0.0642 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 70/150\n",
      "  128/78101 [..............................] - ETA: 35:21 - loss: 0.0651 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 71/150\n",
      "  128/78101 [..............................] - ETA: 32:14 - loss: 0.0646 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 72/150\n",
      "  128/78101 [..............................] - ETA: 31:47 - loss: 0.0662 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 73/150\n",
      "  128/78101 [..............................] - ETA: 31:51 - loss: 0.0672 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 74/150\n",
      "  128/78101 [..............................] - ETA: 31:53 - loss: 0.0682 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 75/150\n",
      "  128/78101 [..............................] - ETA: 31:45 - loss: 0.0620 - accuracy: 0.9877 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 76/150\n",
      "  128/78101 [..............................] - ETA: 31:54 - loss: 0.0683 - accuracy: 0.9853 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 77/150\n",
      "  128/78101 [..............................] - ETA: 31:40 - loss: 0.0644 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 78/150\n",
      "  128/78101 [..............................] - ETA: 31:57 - loss: 0.0665 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 79/150\n",
      "  128/78101 [..............................] - ETA: 31:28 - loss: 0.0622 - accuracy: 0.9873 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 80/150\n",
      "  128/78101 [..............................] - ETA: 31:48 - loss: 0.0676 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 81/150\n",
      "  128/78101 [..............................] - ETA: 31:43 - loss: 0.0634 - accuracy: 0.9872 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 82/150\n",
      "  128/78101 [..............................] - ETA: 31:47 - loss: 0.0634 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 83/150\n",
      "  128/78101 [..............................] - ETA: 31:42 - loss: 0.0690 - accuracy: 0.9852 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 84/150\n",
      "  128/78101 [..............................] - ETA: 30:10 - loss: 0.0648 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00average min val_loss: 0.06618080392608106 -- epochs: [96, 84] -- time: 722.93 seconds\n",
      "{'lstm_layer_size': 256, 'lstm_dropout': 0.2, 'num_mid_dense': 0, 'output_dim': 100}\n",
      "Train on 78101 samples, validate on 19526 samples\n",
      "Epoch 1/150\n",
      "  128/78101 [..............................] - ETA: 3:10:01 - loss: 0.6942 - accuracy: 0.4705 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 2/150\n",
      "  128/78101 [..............................] - ETA: 26:43 - loss: 0.6772 - accuracy: 0.7927 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 3/150\n",
      "  128/78101 [..............................] - ETA: 26:32 - loss: 0.6572 - accuracy: 0.9160 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 4/150\n",
      "  128/78101 [..............................] - ETA: 26:12 - loss: 0.6285 - accuracy: 0.9614 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 5/150\n",
      "  128/78101 [..............................] - ETA: 26:51 - loss: 0.5770 - accuracy: 0.9745 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 6/150\n",
      "  128/78101 [..............................] - ETA: 27:10 - loss: 0.4662 - accuracy: 0.9773 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 7/150\n",
      "  128/78101 [..............................] - ETA: 27:17 - loss: 0.3529 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 8/150\n",
      "  128/78101 [..............................] - ETA: 28:18 - loss: 0.2686 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 9/150\n",
      "  128/78101 [..............................] - ETA: 28:45 - loss: 0.1967 - accuracy: 0.9858 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 10/150\n",
      "  128/78101 [..............................] - ETA: 30:06 - loss: 0.1474 - accuracy: 0.9866 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 11/150\n",
      "  128/78101 [..............................] - ETA: 30:07 - loss: 0.1114 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 12/150\n",
      "  128/78101 [..............................] - ETA: 30:16 - loss: 0.0921 - accuracy: 0.9864 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 13/150\n",
      "  128/78101 [..............................] - ETA: 30:19 - loss: 0.0799 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 14/150\n",
      "  128/78101 [..............................] - ETA: 30:21 - loss: 0.0764 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 15/150\n",
      "  128/78101 [..............................] - ETA: 29:48 - loss: 0.0696 - accuracy: 0.9873 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 16/150\n",
      "  128/78101 [..............................] - ETA: 29:28 - loss: 0.0763 - accuracy: 0.9860 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 17/150\n",
      "  128/78101 [..............................] - ETA: 29:48 - loss: 0.0735 - accuracy: 0.9869 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 18/150\n",
      "  128/78101 [..............................] - ETA: 29:50 - loss: 0.0748 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 19/150\n",
      "  128/78101 [..............................] - ETA: 31:51 - loss: 0.0797 - accuracy: 0.9862 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 20/150\n",
      "  128/78101 [..............................] - ETA: 30:06 - loss: 0.0830 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 21/150\n",
      "  128/78101 [..............................] - ETA: 30:28 - loss: 0.0774 - accuracy: 0.9863 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 22/150\n",
      "  128/78101 [..............................] - ETA: 31:26 - loss: 0.0764 - accuracy: 0.9859 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 23/150\n",
      "  128/78101 [..............................] - ETA: 29:39 - loss: 0.0752 - accuracy: 0.9870 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00Epoch 24/150\n",
      "  128/78101 [..............................] - ETA: 30:26 - loss: 0.0918 - accuracy: 0.9848 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-082b19f4235e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mall_hists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_es\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_create_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_hists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Dokumente\\Studium\\TUM\\Kurse\\Informatik\\Semester 5\\ADNLP\\so_nlp\\toolbox\\training.py\u001b[0m in \u001b[0;36mgrid_search_es\u001b[1;34m(X, y, create_model, search_params, max_epochs)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             hist = model.fit(X_train, y_train, batch_size=128, validation_data=[X_test, y_test], epochs=max_epochs,\n\u001b[1;32m---> 84\u001b[1;33m                              callbacks=callbacks)\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mhists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    504\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    504\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox.training import grid_search_es\n",
    "\n",
    "search_params = {\n",
    "    # conduct big grid search with these params\n",
    "    \"lstm_layer_size\": [256,128],\n",
    "    \"lstm_dropout\": [0.0,0.2,0.4],\n",
    "    \"num_mid_dense\": [1, 0],\n",
    "    \n",
    "    # test grid search with these params (comment out for actual run)\n",
    "    #\"lstm_layer_size\": [16],\n",
    "    #\"lstm_dropout\": [0.0],\n",
    "    #\"num_mid_dense\": [1, 0],\n",
    "    # don#t change\n",
    "    \"output_dim\": [y.shape[-1]]\n",
    "}\n",
    "\n",
    "all_hists = grid_search_es([X_b, X_t], y, tb_create_model, search_params, max_epochs=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our model-paramter to the results of the grid search. Feel free to run it on your own or set parameters manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'lstm_layer_size': 256, 'lstm_dropout': 0.0, 'num_mid_dense': 1, 'output_dim': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best combindation: {'lstm_layer_size': 128, 'lstm_dropout': 0.4, 'num_mid_dense': 1, 'output_dim': 100}\n",
      "avg min val_loss: 0.0663479853600407 -- epoch counts: [100, 93]\n"
     ]
    }
   ],
   "source": [
    "# Chooses the parameter configuration, which lead to the lowest loss during the grid search\n",
    "best_params, best_hist, best_loss = min(all_hists, key=lambda x: x[2])\n",
    "\n",
    "epoch_lengths = [len(h[\"val_loss\"]) for h in best_hist]\n",
    "print(f\"best combindation: {best_params}\")\n",
    "print(f\"avg min val_loss: {best_loss} -- epoch counts: {epoch_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78021, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train and test split with two datasets. Requires the zip to make the split for the question body and the question title\n",
    "X_train_z, X_test_z, y_train, y_test = train_test_split(list(zip(X_b, X_t)), y, test_size=0.2)\n",
    "X_train = list(zip(*X_train_z))\n",
    "X_test = list(zip(*X_test_z))\n",
    "#Convert to numpy arrays \n",
    "X_train=[np.array(X_train[0]), np.array(X_train[1])]\n",
    "X_test=[np.array(X_test[0]), np.array(X_test[1])]\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 100)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          365568      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          12900       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 809,700\n",
      "Trainable params: 809,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tb_create_model(**best_params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78021 samples, validate on 19506 samples\n",
      "Epoch 1/300\n",
      "78021/78021 [==============================] - 147s 2ms/sample - loss: 0.0694 - accuracy: 0.9840 - val_loss: 0.0505 - val_accuracy: 0.9878\n",
      "Epoch 2/300\n",
      "78021/78021 [==============================] - 34s 438us/sample - loss: 0.0450 - accuracy: 0.9883 - val_loss: 0.0410 - val_accuracy: 0.9887\n",
      "Epoch 3/300\n",
      "78021/78021 [==============================] - 31s 403us/sample - loss: 0.0383 - accuracy: 0.9893 - val_loss: 0.0365 - val_accuracy: 0.9897\n",
      "Epoch 4/300\n",
      "78021/78021 [==============================] - 32s 408us/sample - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.0341 - val_accuracy: 0.9902\n",
      "Epoch 5/300\n",
      "78021/78021 [==============================] - 32s 416us/sample - loss: 0.0326 - accuracy: 0.9905 - val_loss: 0.0326 - val_accuracy: 0.9906\n",
      "Epoch 6/300\n",
      "78021/78021 [==============================] - 33s 426us/sample - loss: 0.0312 - accuracy: 0.9909 - val_loss: 0.0317 - val_accuracy: 0.9908\n",
      "Epoch 7/300\n",
      "78021/78021 [==============================] - 33s 427us/sample - loss: 0.0301 - accuracy: 0.9912 - val_loss: 0.0310 - val_accuracy: 0.9910\n",
      "Epoch 8/300\n",
      "78021/78021 [==============================] - 33s 429us/sample - loss: 0.0292 - accuracy: 0.9914 - val_loss: 0.0305 - val_accuracy: 0.9912\n",
      "Epoch 9/300\n",
      "78021/78021 [==============================] - 34s 440us/sample - loss: 0.0285 - accuracy: 0.9916 - val_loss: 0.0303 - val_accuracy: 0.9913\n",
      "Epoch 10/300\n",
      "78021/78021 [==============================] - 35s 452us/sample - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0298 - val_accuracy: 0.9914\n",
      "Epoch 11/300\n",
      "78021/78021 [==============================] - 35s 449us/sample - loss: 0.0272 - accuracy: 0.9919 - val_loss: 0.0298 - val_accuracy: 0.9914\n",
      "Epoch 12/300\n",
      "78021/78021 [==============================] - 35s 449us/sample - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.0291 - val_accuracy: 0.9915\n",
      "Epoch 13/300\n",
      "78021/78021 [==============================] - 37s 474us/sample - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.0289 - val_accuracy: 0.9915\n",
      "Epoch 14/300\n",
      "78021/78021 [==============================] - 36s 464us/sample - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0285 - val_accuracy: 0.9917\n",
      "Epoch 15/300\n",
      "78021/78021 [==============================] - 35s 446us/sample - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0282 - val_accuracy: 0.9917\n",
      "Epoch 16/300\n",
      "78021/78021 [==============================] - 34s 434us/sample - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0281 - val_accuracy: 0.9917\n",
      "Epoch 17/300\n",
      "78021/78021 [==============================] - 36s 456us/sample - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0284 - val_accuracy: 0.9917\n",
      "Epoch 18/300\n",
      "78021/78021 [==============================] - 35s 443us/sample - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.0282 - val_accuracy: 0.9917\n",
      "Epoch 19/300\n",
      "78021/78021 [==============================] - 34s 441us/sample - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.0283 - val_accuracy: 0.9918\n",
      "Epoch 20/300\n",
      "78021/78021 [==============================] - 36s 459us/sample - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0287 - val_accuracy: 0.9917\n",
      "Epoch 21/300\n",
      "78021/78021 [==============================] - 36s 457us/sample - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.0290 - val_accuracy: 0.9916\n",
      "Epoch 22/300\n",
      "78021/78021 [==============================] - 36s 466us/sample - loss: 0.0186 - accuracy: 0.9937 - val_loss: 0.0291 - val_accuracy: 0.9917\n",
      "Epoch 23/300\n",
      "78021/78021 [==============================] - 35s 454us/sample - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.0296 - val_accuracy: 0.9916\n",
      "Epoch 24/300\n",
      "78021/78021 [==============================] - 35s 444us/sample - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.0298 - val_accuracy: 0.9916\n",
      "Epoch 25/300\n",
      "78021/78021 [==============================] - 35s 444us/sample - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.0307 - val_accuracy: 0.9914\n",
      "Epoch 26/300\n",
      "78021/78021 [==============================] - 37s 471us/sample - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.0315 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2558f607808>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are working on linux or windows, pay attention to the file paths \"/\" and \"\\\\\" respectively\n",
    "\n",
    "import datetime\n",
    "\n",
    "model_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir=\"logs\\\\fit\\\\\" + model_name\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0, restore_best_weights=True),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    # ModelCheckpoint(filepath=f\"checkpoints\\\\{model_name}\", monitor=\"val_loss\", verbose=0)\n",
    "]\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=128, epochs = 300, validation_data=[X_test, y_test], callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "\n",
      "normalize_embeddings = True, learning_rate = 1, vocab_size = None, epochs=300\n",
      "Parameter Settings:\n",
      " Sample size = -1, Max. number of words per question = 100, Number of Top Labels used = 100\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 100)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          365568      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          12900       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 809,700\n",
      "Trainable params: 809,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics with optimized threshold of 0.38\n",
      " Macro Evaluation: f1_Score= 0.5476427858927477 , Recall = 0.5003593326350425 , Precision = 0.6480929682937218\n",
      " Micro Evaluation: f1_Score= 0.6468479423048702 , Recall = 0.5746265877006742 , Precision = 0.7398331095338576\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "opt_thres = output_evaluation(model, sample_size, max_question_words, n_top_labels, y_test, predictions, normalize_embeddings, 1, None, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40, 100)\n",
      "(100, 100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "X_val_t = validation_questions[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_val_b = validation_questions[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "X_val_t = pad_sequences(X_val_t, padding=\"post\", dtype='float32', value=padding_element, maxlen=X_t.shape[1])\n",
    "X_val_b = pad_sequences(X_val_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "\n",
    "print(X_val_t.shape)\n",
    "print(X_val_b.shape)\n",
    "\n",
    "y_val = label_encoder.transform(validation_questions[\"tags\"])\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: ('file',)\n",
      "PREDICTION: ()\n",
      "\n",
      "Homework - Printing lines of a file between two line numbers\n",
      "\n",
      "Using Python, how do I print the lines of a text file, given a starting and ending line number?\n",
      "I have come up with a function, but this doesn't work.\n",
      "def printPart(src, des, varFile):\n",
      "    returnLines = \"\"\n",
      "    for curLine in range(src, des):\n",
      "        returnLines += linecache.getline(varFile, curLine)\n",
      "    return returnLines\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pandas',)\n",
      "PREDICTION: ('matplotlib', 'pandas')\n",
      "\n",
      "Plot series different frequencies\n",
      "\n",
      "I have some time series, for which I created both quarterly and annual sums. I wanted to plot the two together, but the plot only showed the more frequent one. merged.plot() gives:\n",
      "\n",
      "Here's some sample data.\n",
      ">>> merged.head()\n",
      "           shopping_weightedQ             shopping_weightedA            \n",
      "status                    emp       unemp                emp       unemp\n",
      "date                                                                    \n",
      "2003-01-01         653.964346  696.178441         695.374248  755.180039\n",
      "2003-04-01         702.233863  647.461856                NaN         NaN\n",
      "2003-07-01         665.619252  774.226719                NaN         NaN\n",
      "2003-10-01         757.189689  932.586052                NaN         NaN\n",
      "2004-01-01         670.114570  751.718014         703.479640  839.858502\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django', 'django-models')\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "Django: find foreign key by class\n",
      "\n",
      "I have a bunch of Django models with a foreign key to an object of class Foo. They use different names for the foreign key -- i.e. it isn't always foo = models.ForeignKey(Foo); it could be bar = models.ForeignKey(Foo). I would like a method that can retrieve the Foo object, regardless of the name of the field. The logic would be \"find the single ForeignKey that points to a Foo object, and get that object.\n",
      "Is this possible?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays', 'numpy', 'python-2.7')\n",
      "PREDICTION: ('arrays', 'numpy')\n",
      "\n",
      "Python: Saving / loading large array using numpy\n",
      "\n",
      "I have saved a large array of complex numbers using python,\n",
      "numpy.save(file_name, eval(variable_name))\n",
      "\n",
      "that worked without any trouble. However, loading,\n",
      "variable_name=numpy.load(file_name)\n",
      "\n",
      "yields the following error,\n",
      "ValueError: total size of new array must be unchanged\n",
      "\n",
      "Using: Python 2.7.9 64-bit and the file is 1.19 GB large.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('sorting',)\n",
      "PREDICTION: ('file',)\n",
      "\n",
      "python file content sorting\n",
      "\n",
      "My file content is below:\n",
      "Dnext_0[11]\n",
      "Dnext_1[0]\n",
      "Dnext_0[0]\n",
      "Dnext_0[128]\n",
      "Dnext_0[1]\n",
      "\n",
      "After sorting, I want to have:\n",
      "Dnext_0[0]\n",
      "Dnext_0[1]\n",
      "Dnext_0[11]\n",
      "Dnext_0[128]\n",
      "Dnext_1[0]\n",
      "\n",
      "I have tried this:\n",
      "with open('testfile') as f:\n",
      "    sorted_file = sorted(f)\n",
      "\n",
      "print sorted_file\n",
      "\n",
      "But it does not give me what I want.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('for-loop',)\n",
      "PREDICTION: ('for-loop', 'loops')\n",
      "\n",
      "Python: Nested For Loops\n",
      "\n",
      "I have a question I need to ask you. Here is part of my script:\n",
      "scan=file('indice.txt','r')\n",
      "for i_L in range(10):\n",
      "    for line in scan:\n",
      "        a,b,c=line.split()\n",
      "        do something ...\n",
      "        ... ...\n",
      "    print something\n",
      "scan.close()\n",
      "\n",
      "indice.txt is a file containing 3 columns of real numbers. \n",
      "The main problem is that when the outer loop over i_L executes for the first value of i_L, the loop is broken and only one value is shown in the output.\n",
      "anyone can help?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('regex',)\n",
      "PREDICTION: ('regex',)\n",
      "\n",
      "How to make python regex which matches multiple patterns to same index\n",
      "\n",
      "Is  it possible to get all overlapping matches, which starts from the same index, but are from different matching group? \n",
      "e.g. when I look for pattern \"(A)|(AB)\" from \"ABC\" regex should return following matches:\n",
      "(0,\"A\") and (0,\"AB\")\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ()\n",
      "\n",
      "How to implement GtkOrientable in my custom widget\n",
      "\n",
      "How can I implement GtkOrientable in my custom widget, what I've done so far is:\n",
      "class MyOwnWidget(Gtk.Orientable, Gtk.DrawingArea):\n",
      "    ...\n",
      "\n",
      "when I run Gtk throws:\n",
      "/usr/lib/python3.4/site-packages/gi/types.py:194: Warning: Object class gtkmeter+GtkMeter doesn't implement property 'orientation' from interface 'GtkOrientable'\n",
      "_gobject.type_register(cls, namespace.get('__gtype_name__'))\n",
      "\n",
      "so which are the correct steps to implement GtkOrientable ?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pip', 'python-3.x')\n",
      "PREDICTION: ()\n",
      "\n",
      "When import docx in python3.3 I have error ImportError: No module named 'exceptions'\n",
      "\n",
      "when I import docx I have this error:\n",
      ">File \"/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/docx-0.2.4-py3.3.egg/docx.py\", line 30, in <module>\n",
      "        from exceptions import PendingDeprecationWarning\n",
      "    ImportError: No module named 'exceptions'\n",
      "\n",
      "How to fix this error (python3.3, docx 0.2.4)?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('tkinter',)\n",
      "PREDICTION: ('tkinter',)\n",
      "\n",
      "pyinstaller executable takes more than 8 minutes print out?\n",
      "\n",
      "I have code which runs immediately when I run using python. The code has tkinter module and bunch of if statements. I created a standalone executable and it takes about 8 minutes to give the output for the GUI. I was wondering why it takes so much time to run ?\n",
      "Thanks in advance.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('loops', 'pyqt')\n",
      "PREDICTION: ('pyqt', 'pyqt4', 'user-interface')\n",
      "\n",
      "Grabbing all QPushButton in a PyQt Python ui File\n",
      "\n",
      "I created a UI File using Qt Designer with lots of QPushButtons, and then I converted it into a python file using pyuic4.\n",
      "I want to add all the QPushButtons to a QButtonGroup.\n",
      "How do I iterate or grab all my QPushButtons to add in to a QButtonGroup from my UI Python file?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays', 'list', 'python-2.7', 'string')\n",
      "PREDICTION: ('list',)\n",
      "\n",
      "Python - Select all elements a list of a list\n",
      "\n",
      "I want to write a series of code (it may be func, loop or etc.) to get first 6 chars of each list of every list.\n",
      "It looks like this:\n",
      "http://www.mackolik.com/AjaxHandlers/FixtureHandler.aspx?command=getMatches&id=3170&week=1\n",
      "this is the first list of my list, second can be found here: week=2.\n",
      "It goes through 11.\n",
      "In addition to this, each list element of my list differentiates.\n",
      "Can you help me or give an idea to deal with.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('shell', 'windows')\n",
      "PREDICTION: ('windows',)\n",
      "\n",
      "Capture all output and error, warning of a command in windows by python\n",
      "\n",
      "In bash shell of Linux, I can read a command (from file), then execute the command and write all the output, error, and return code to a file. Can I do that by using python in windows.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ('list', 'python-3.x')\n",
      "\n",
      "Python3 How to make a bytes object from a list of integers\n",
      "\n",
      "I have an array of integers (all less than 255) that correspond to byte values  (i.e. [55, 33, 22]) how can I turn that into a bytes object that would look like \n",
      "b'\\x55\\x33\\x22  etc.\n",
      "Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('logging',)\n",
      "PREDICTION: ('logging',)\n",
      "\n",
      "how to log background python script's output if some exception is not catched\n",
      "\n",
      "I have a python script running in background, and I want to log all the exception and output to a log file.\n",
      "I know to use logging module and try.. catch.. to log exception, but what if I missed some, is there any way to log these exceptions too?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pandas',)\n",
      "PREDICTION: ('pandas',)\n",
      "\n",
      "Convert python pandas rows to columns\n",
      "\n",
      "        Decade           difference (kg)    Version\n",
      "0  1510 - 1500                  -0.346051   v1.0h\n",
      "1  1510 - 1500                  -3.553251   A2011\n",
      "2  1520 - 1510                  -0.356409   v1.0h\n",
      "3  1520 - 1510                  -2.797978   A2011\n",
      "4  1530 - 1520                  -0.358922   v1.0h\n",
      "\n",
      "I want to transform the pandas dataframe such that the 2 unique enteries in the Version column are transfered to become columns. How do I do that?\n",
      "The resulting dataframe should not have a multiindex\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-2.7', 'python-3.x', 'tkinter')\n",
      "PREDICTION: ('tkinter',)\n",
      "\n",
      "TkInter install using \"easy Install\"\n",
      "\n",
      "Hi i'm trying to write some project with a friend of mine using Cloud9.\n",
      "In my project i need the module TkInter.\n",
      "Cloud9 supports easyInstall.\n",
      "Does anyone know how to install tkinter in cloud9 or at least where to get the package for easyInstall?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('dictionary',)\n",
      "PREDICTION: ('dictionary',)\n",
      "\n",
      "put top 2 biggest keys and values of dictionary in another dictionary\n",
      "\n",
      "Here is my dictionary:\n",
      "d = {'a': 100, 'b': 200, 'c': 300, 'd': 350} \n",
      "\n",
      "I can find top 2 keys with biggest values and put them to the list:\n",
      "sorted(d, key=d.get, reverse=True)[:2]\n",
      "\n",
      "But what should I do to put top 2 biggest keys and values in another dictionary instead of list?\n",
      "Thanks.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "Best way to Sign in to Twitter with Django\n",
      "\n",
      "I use these 3 ways to login twitter on django, but not successful now:\n",
      "\n",
      "Django-SocialAuth\n",
      "django-twitterauth\n",
      "http://hameedullah.com/step-by-step-guide-to-use-sign-in-with-twitter-with-django.html\n",
      "\n",
      "I'm tired with it, so are there any other ways to sign in to Twitter with Django.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('datetime',)\n",
      "PREDICTION: ('datetime',)\n",
      "\n",
      "Python converting string date object to date gives valuerror\n",
      "\n",
      "I'm trying to convert string date object to date object in python.\n",
      "I did this so far \n",
      " old_date = '01 April 1986' \n",
      " new_date = datetime.strptime(old_date,'%d %M %Y')\n",
      " print new_date\n",
      "\n",
      "But I get the following error.\n",
      "\n",
      "ValueError: time data '01 April 1986' does not match format '%d %M %Y'\n",
      "\n",
      "Any guess?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django', 'django-admin')\n",
      "\n",
      "How to populate a custom django admin change_form.html foreignkey field in select tag\n",
      "\n",
      "I have arranged the templates of my project in the correct order so that I can inherit the django's admin change_form.html template for my model. Now I wish to customize the foreignkey field which is a SELECT dropdown. This is editable in case of superuser login and is just a select option for technical login. Based on selected value, wish to hide or display some of the other fields on the form. Can anyone give me a detailed example of how this can be achieved? I wish to use javascript.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django', 'pip')\n",
      "\n",
      "How does one automate the install of lessc using pip and setup.py?\n",
      "\n",
      "I have a django project that uses the django-static-precompiler, which required lessc (a css pre-processor) for converting LESS files into CSS.\n",
      "Unfortunately, lessc is not a Python program, and the django-static-precompiler documentation doesn't give tips on how to include this automatically.\n",
      "I'd like a user to be able to run setup.py install and be at a point where the system is relatively functional, but this is a small hurdle.\n",
      "What can I put in setup.py to require the install of lessc, or any non-pip available package for that matter?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('wxpython',)\n",
      "PREDICTION: ('wxpython',)\n",
      "\n",
      "Is it is possible to \"set\" a list to a ComboBox, wxpython?\n",
      "\n",
      "Hi i know its possible to do this with lists however is it possible to do this with Comboboxes? Is there anything similar to the set function?\n",
      "I have tried using set with a Combo box but i receive the following error:\n",
      "    AttributeError: 'ComboBox' object has no attribute 'Set'\n",
      "Thanks.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-2.7', 'urllib2')\n",
      "PREDICTION: ('urllib2',)\n",
      "\n",
      "Python urllib2 read return different results for a ftp request, why?\n",
      "\n",
      "I ran into some werid behavior, these two functions echo different sizes always. The first one echoes certain and right result, while the second one\n",
      "echoes random wrong results, is there anyone who knows why?  I am using Python 2.7.10.\n",
      "import urllib2\n",
      "\n",
      "url = 'ftp://ftp.ripe.net/pub/stats/ripencc/delegated-ripencc-extended-latest'\n",
      "\n",
      "def one():\n",
      "    response = urllib2.urlopen(url)\n",
      "    data = response.read()\n",
      "    print len(data)\n",
      "\n",
      "def two():\n",
      "    print len(urllib2.urlopen(url).read())\n",
      "\n",
      "one()\n",
      "two()\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django', 'json')\n",
      "PREDICTION: ('json',)\n",
      "\n",
      "How to validate JSON with simplejson\n",
      "\n",
      "Occasionally I'm querying a server for JSON and receiving a 404 HTML page when the requested data is not available.  \n",
      "Thus, I must have a check to ensure that the JSON I'm expecting, is actually json rather than HTML.  I'm accomplishing this now by checking for a string that I can expect to be in the HTML is contained in the response, but I think there has to be a better way to do this.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-2.7',)\n",
      "PREDICTION: ()\n",
      "\n",
      "ANTLR4: How to generate a graphic of the parse tree when using a Python target?\n",
      "\n",
      "When using Java-ANTLR I can use the testrig with the -gui flag and generate a nice graphic of the parse tree.\n",
      "I am now using Python-ANTLR. Is there a testrig available with a -gui flag that allows me to see a nice graphic of the parse tree?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('shell',)\n",
      "PREDICTION: ('bash', 'shell')\n",
      "\n",
      "Shell Script: Execute a python program from within a shell script\n",
      "\n",
      "I've tried googling the answer but with no luck.\n",
      "I need to use my works supercomputer server, but for my python script to run, it must be executed via a shell script.\n",
      "For example I want job.sh to execute python_script.py\n",
      "How can this be accomplished?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('opencv', 'python-2.7')\n",
      "PREDICTION: ()\n",
      "\n",
      "How do I solve this issue for this Multiscale Template Matching script?\n",
      "\n",
      "I didnât have a lot of characters to write the question and description so I uploaded a file on dropbox to explain the issue:\n",
      "https://www.dropbox.com/s/0479x17iljd8fi9/TemplateMatchIssue%20%26%20Code.txt?dl=0\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('list',)\n",
      "PREDICTION: ('list',)\n",
      "\n",
      "Slice list to ordered chunks\n",
      "\n",
      "I have dictionary like:\n",
      "item_count_per_section = {1: 3, 2: 5, 3: 2, 4: 2}\n",
      "\n",
      "And total count of items retrieved from this dictionary:\n",
      "total_items = range(sum(item_count_per_section.values()))\n",
      "\n",
      "Now I want to transform total_items by values of dictionary following way:\n",
      "items_no_per_section = {1: [0,1,2], 2: [3,4,5,6,7], 3:[8,9], 4:[10,11] }\n",
      "\n",
      "I.e. slice total_items sequencially to sublists which startrs from previous \"iteration\" index and finished with value from initial dictionary.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('sqlalchemy', 'sqlite')\n",
      "PREDICTION: ('sqlite',)\n",
      "\n",
      "How do I escape the - character in SQLite FTS3 queries?\n",
      "\n",
      "I'm using Python and SQLAlchemy to query a SQLite FTS3 (full-text) store and I would like to prevent my users from using the - as an operator. How should I escape the - so users can search for a term containing the - (enabled by changing the default tokenizer) instead of it signifying \"does not contain the term following the -\"?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django', 'http')\n",
      "\n",
      "How to make an HTTP request into an application within an opened session? (Django/Python)\n",
      "\n",
      "Here's the deal: I've modified an open-source platform API to retrieve some information I needed (some basic info of registered users to create a user profile page). The user must be authenticated to use it.\n",
      "I need to make a request to the API to retrieve that info, but how do I do it within that session? I'm using Python's request method into my page, but apparently it creates a new session, since I'm getting 401 errors whenever I use it.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('database', 'django')\n",
      "PREDICTION: ('django', 'django-models')\n",
      "\n",
      "Python/Django Modeling Question\n",
      "\n",
      "What is the best way to have many children records pointing to one parent record in the same model/table in Django?\n",
      "Is this implementation correct?:\n",
      "class TABLE(models.Model):\n",
      "    id = models.AutoField(primary_key=True)\n",
      "    parent = models.ForeignKey(\"TABLE\", unique=False)\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('tkinter', 'user-interface')\n",
      "PREDICTION: ('tkinter',)\n",
      "\n",
      "Python tkinter difference between spinbox and optionmenu\n",
      "\n",
      "I was just wondering what the difference between a spin box and option menu is?\n",
      "They both seem to be a drop down list displaying different options. \n",
      "I'm a beginner in programming, so will appreciate it if someone could explain without going into too much technical details.\n",
      "Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays', 'performance')\n",
      "PREDICTION: ('arrays',)\n",
      "\n",
      "which is more efficient for buffer manipulations: python strings or array()\n",
      "\n",
      "I am building a routine that processes disk buffers for forensic purposes. Am I better off using python strings or the array() type? My first thought was to use strings, but I'm trying to void unicode problems, so perhaps array('c') is better?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib',)\n",
      "PREDICTION: ()\n",
      "\n",
      "Having issues with matplolib for loop getting slower\n",
      "\n",
      "I want to loop over a series of images to see how they change over time. Thus, I want them plotted on the same figure. The following code works but seems to slow down after a few iterations. Does anyone know why this is happening, how to overcome it, or an alternative way to visualize these images over time?\n",
      "fig, ax=pyplot.subplots(figsize=(8,6))\n",
      "for i in range(n):\n",
      "    ax.imshow(imageArray[i])\n",
      "    fig.canvas.draw()\n",
      "    time.sleep(0.2)\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('multithreading',)\n",
      "PREDICTION: ('multithreading',)\n",
      "\n",
      "Python epoll.register threadsafe?\n",
      "\n",
      "Does anyone know if I can call epoll.register from another thread safely?\n",
      "Here is what I am imagining:\n",
      "\n",
      "Thread 1: epoll.poll()\n",
      "Thread 2: adding some fd to the same epoll object with epoll.register\n",
      "\n",
      "http://docs.python.org/library/select.html\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib',)\n",
      "PREDICTION: ('matplotlib',)\n",
      "\n",
      "python matplotlib.plot does not show chart\n",
      "\n",
      "Have recently started playing with Python. I have installed Python Spyder app on the Mac. Everything has been working well until recently for some reason the matplotlib charts stopped displaying. A simple code like this does  not work anymore:\n",
      "import matplotlib.pyplot as plt\n",
      "x = np.linspace(0.4 * np.pi, 100)\n",
      "plt.plot(x)\n",
      "\n",
      "The only output is \n",
      "    []\n",
      "I have Spyder 2.3.3, Python 3.4.3 64 Bit. \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('database', 'django')\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "executing django query in same statement\n",
      "\n",
      "This might be a silly question. Is there a way to execute multiple queries in one execute statement?\n",
      "cursor = conn.cursor()\n",
      "cursor.execute(\"Select * from my_tables; show tables;\")\n",
      "result = cursor.fetchall()\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ('pip',)\n",
      "\n",
      "Error in trying to install python package\n",
      "\n",
      "I am trying to install python-master-recsys package and i get the error as follows:\n",
      "File \"C:\\Users\\Dixon\\AppData\\Local\\Temp\\easy_install-s6dnujjk\\csc-pysparse-1.1.1.4\\setup.py\", line 33\n",
      "    print 'setuptools module not found.'\n",
      "                                       ^\n",
      "SyntaxError: Missing parentheses in call to 'print'\n",
      "\n",
      "How do i debug this error? I am using python 3.5\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-imaging-library',)\n",
      "PREDICTION: ('image', 'python-imaging-library')\n",
      "\n",
      "How to get image size in python-pillow after resize?\n",
      "\n",
      "resized_image = Image.resize((100,200));\n",
      "Image is Python-Pillow Image class, and i've used the resize function to resize the original image,\n",
      "How do i find the new file-size (in bytes) of the resized_image without having to save to disk and then reading it again \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('list', 'tuples')\n",
      "PREDICTION: ('list', 'tuples')\n",
      "\n",
      "How to remove the last element in each tuple in a list\n",
      "\n",
      "I've got a list like:\n",
      "alist = [[a,b,(1,2)], [a,b,(1,2)], [a,b,(1,2)]]\n",
      "\n",
      "I want to remove the last element from all the elements in a list. So the result will be:\n",
      "alist = [[a,b], [a,b], [a,b]]\n",
      "\n",
      "Is there a fast way to do this?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays', 'numpy')\n",
      "PREDICTION: ('arrays', 'numpy')\n",
      "\n",
      "Numpy - array vs asarray\n",
      "\n",
      "What is the difference between Numpy's array() and asarray() functions? When should you use one rather than the other? They seem to generate identical output for all the inputs I can think of.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('ubuntu',)\n",
      "PREDICTION: ('bash',)\n",
      "\n",
      "How to redirect output of Python script to terminal\n",
      "\n",
      "How can I redirect my Python script output to one of the open terminal windows under Ubuntu? The script is spawned by KMail filtering rule.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-requests',)\n",
      "PREDICTION: ('python-requests',)\n",
      "\n",
      "update cookies in session using python-requests module\n",
      "\n",
      "I'm using python-requests module to handle oAuth request and response.\n",
      "I want to set received access_token (response content as dict) in requests.session.cookies object.\n",
      "How can I update existing cookies of session with received response from server ?\n",
      "[EDIT]\n",
      "self.session = requests.session(auth=self.auth_params)\n",
      "resp = self.session.post(url, data=data, headers=self.headers)\n",
      "content = resp.content\n",
      "\n",
      "I want to do something like :\n",
      "requests.utils.dict_from_cookiejar(self.session.cookies).update(content)\n",
      "here, requests.utils.dict_from_cookiejar(self.session.cookies) returns dict with one session key.\n",
      "now, I want to update received response.content in self.session.cookies.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('windows',)\n",
      "PREDICTION: ('linux',)\n",
      "\n",
      "Detect how I run Python script\n",
      "\n",
      "How do I detect if a script is run from a Windows console or from Komodo debugger without passing different arguments to the script?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('json',)\n",
      "PREDICTION: ('datetime', 'parsing')\n",
      "\n",
      "How to parse Date(928142400000+0200)?\n",
      "\n",
      "I have JSON response object with string representing date and time:\n",
      "\"event\":{\n",
      "    \"type\":\"Type\",\n",
      "    \"date-time\":\"\\/Date(928142400000+0200)\\/\",\n",
      "},\n",
      "\n",
      "I am not sure:\n",
      "\n",
      "what format is that\n",
      "how can I parse it in python app\n",
      "how can I convert python date into this format\n",
      "\n",
      "Any suggestions?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('html', 'parsing')\n",
      "PREDICTION: ('beautifulsoup', 'html', 'parsing')\n",
      "\n",
      "Python parsing html mismatched tag error\n",
      "\n",
      "30   <li class=\"start_1\">\n",
      "31               <input type=\"checkbox\" name=\"word_ids[]\" value=\"34\" class=\"list_check\">\n",
      "32          </li> \n",
      "\n",
      "This is a part of html file that I want to parse. But when I applied \n",
      "uh = open('1.htm','r')\n",
      "data = uh.read()\n",
      "print data  \n",
      "tree = ET.fromstring(data)\n",
      "\n",
      "It showed\n",
      "\n",
      "xml.etree.ElementTree.ParseError: mismatched tag: line 32, column 18\n",
      "\n",
      "I don't know what is going wrong?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pyqt', 'qt')\n",
      "PREDICTION: ('pyqt', 'qt')\n",
      "\n",
      "Catching a drag exit in Qt?\n",
      "\n",
      "I've got a custom widget descended from QWidget that I want to be able to drop onto, and while the drag is hovering over the widget I'd like to highlight it to provide a little visual feedback to the user.   Seems to me the simplest way to do this would be to highlight when dragEnterEvent is called and unhighlight when the drag exits the widget, but how can I catch the drag exit?  There doesn't seem to be a dragExitEvent event handler.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays', 'numpy')\n",
      "PREDICTION: ('arrays', 'numpy')\n",
      "\n",
      "Python comparing two 3 dimensional numpy arrays\n",
      "\n",
      "I have two numpy arrays:\n",
      "A.shape = (nA,x,y)\n",
      "\n",
      "and\n",
      "B.shape = (nB,x,y).\n",
      "\n",
      "I want to find all subarrays such that \n",
      "A(i,:,:) == B(j,:,:).\n",
      "\n",
      "I know I can write a double for loop and use\n",
      "np.array_equal(A(i,:,:),B(j,:,:)\n",
      "\n",
      "However, is there a more efficient method?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('dictionary',)\n",
      "PREDICTION: ('dictionary',)\n",
      "\n",
      "How to get the item with max date value in a python dictionary?\n",
      "\n",
      "In dictionary a:\n",
      "from datetime import datetime\n",
      "\n",
      "a = {\"ID\":3, \"CITY\":\"ATLANTIS\", \"FOUNDATION\":datetime(2014,10,12), \"COLLAPSE\":datetime(2010,10,12), \"REGENERATE\":datetime(2011,10,12)}\n",
      "\n",
      "How would you get the value which has the oldest date in this dictionary (in this case \"COLLAPSE\":datetime(2010,10,12)) ? Remember, not all values are of same data type.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('function',)\n",
      "PREDICTION: ('python-3.x',)\n",
      "\n",
      "Why does Python print this?\n",
      "\n",
      "Can someone explain why this Python code:\n",
      "def function(string, i, j): \n",
      "    if (i < j):\n",
      "        i = i+1\n",
      "        string1 = string[i:j] return string1\n",
      "    else:\n",
      "        return string\n",
      "# main\n",
      "string = \"four score and seven years ago\" \n",
      "i = 5\n",
      "j = 9\n",
      "stringslice = function(string, i, j) \n",
      "print (stringslice)\n",
      "print (i, j)\n",
      "\n",
      "prints:\n",
      "cor\n",
      "5 9\n",
      "\n",
      "I thought it should print the following:\n",
      "cor\n",
      "6 9\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "Crop image from django template\n",
      "\n",
      "Is there any way to crop an image from django template? I am not saving the image anywhere it is  URL example. \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('dictionary',)\n",
      "PREDICTION: ('dictionary',)\n",
      "\n",
      "TypeError: 'tuple' object does not support item assignment in dictionary\n",
      "\n",
      "An increment could not be made on the corresponding values of the dictionary elements\n",
      "sentiment_words = {}\n",
      "for word in TotalVector:\n",
      "    if not word in sentiment_words:\n",
      "        sentiment_words[word]=(0,0,0)\n",
      "        #sentiment_word(positive,negative,neutral)\n",
      "    if ispositive(word):\n",
      "        sentiment_words[word][0] += 1\n",
      "    elif isnegative(word):\n",
      "        sentiment_words[word][1] += 1\n",
      "    elif isneutral(word):\n",
      "        sentiment_words[word][2] += 1\n",
      "\n",
      "print sentiment_words\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-2.7',)\n",
      "PREDICTION: ('python-3.x', 'string')\n",
      "\n",
      "Why do I get 4 bytes for a char in python?\n",
      "\n",
      "file1 = open(\"test.txt\", 'wb')\n",
      "file1.write(struct.pack('icic', 1, '\\t', 2, '\\n'))\n",
      "file1.close()\n",
      "print os.path.getsize(\"test.txt\")\n",
      "\n",
      "It gives me 13. But I think it should be 4 + 1 + 4 + 1 = 10 bytes. It seems that it stores '\\n' for one byte, but '\\t' for 4 bytes. And idea?\n",
      "Thanks!\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('json',)\n",
      "PREDICTION: ('json',)\n",
      "\n",
      "Python: escaping issue with ujson\n",
      "\n",
      "I am using ujson to convert dictionary to json.\n",
      "when I run the following line:\n",
      "ujson.dumps({'key':'val\\1'})\n",
      "\n",
      "I get the following result:\n",
      "[{\"key\": \"val\\\\1\"}]\n",
      "\n",
      "while I expect/want it to be:\n",
      "[{\"key\": \"val\\1\"}]\n",
      "\n",
      "any idea?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('ubuntu',)\n",
      "PREDICTION: ()\n",
      "\n",
      "ImportError: No module named foxhound.utils.vis\n",
      "\n",
      "I am using Ubuntu, and python 2.7. Is there anybody help me overcome the following error?\n",
      "\n",
      "ImportError: No module named foxhound.utils.vis\n",
      "\n",
      "which is caused by this line:\n",
      "from foxhound.utils.vis import grayscale_grid_vis, unit_scale\n",
      "\n",
      "I also did:                                              \n",
      "export PYTHONPATH=\"/home/jerome/bin/django-1.1/lib/python2.6/site-packages:$PYTHONPATH\"\n",
      "\n",
      "but no help!!\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('http', 'twisted')\n",
      "PREDICTION: ('http', 'twisted')\n",
      "\n",
      "How to Force File Download on Twisted HTTP Server?\n",
      "\n",
      "Pretty simple question, I am currently wanting for users to download (not open) a csv file that is stored on a python Twisted Web Server. The file currently opens in the browser when accessing its URL.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('html', 'xml')\n",
      "PREDICTION: ('html', 'lxml', 'xml')\n",
      "\n",
      "How to Translate XML into HTML using XSL Stylesheet in Python?\n",
      "\n",
      "I have an XSL stylesheet template that transforms my XML file into HTML. How can I perform such processing using Python?\n",
      "...and here's the link to really simple solution:)\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('excel', 'python-2.7')\n",
      "PREDICTION: ('excel',)\n",
      "\n",
      "how to do excel's 'format as table' in python\n",
      "\n",
      "I'm using xlwt to create tables in excel. In excel there is a feature format as table which makes the table have an automatic filters for each column. Is there a way to do it using python?  \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib',)\n",
      "PREDICTION: ('matplotlib',)\n",
      "\n",
      "Remove or adapt border of frame of legend using matplotlib\n",
      "\n",
      "When plotting a plot using matplotlib:\n",
      "\n",
      "How to remove the box of the legend? \n",
      "How to change the color of the border of the legend box?\n",
      "How to remove only the border of the box of the legend?\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('regex',)\n",
      "PREDICTION: ('regex',)\n",
      "\n",
      "Python findall, regex\n",
      "\n",
      "I have this text: \n",
      "  u'times_viewed': 12268,\n",
      "  u'url': u'/photo/79169307/30-seconds-light',\n",
      "  u'user': {u'affection': 63962,\n",
      "\n",
      "How can I just get out this string: \"/photo/79169307/30-seconds-light\"?\n",
      "I am trying with regex and findall:\n",
      "list = re.findall(âuâurlâ: uâ/photo/\"([^\"]*)\"â, text)\n",
      "\n",
      "but it won't go.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('beautifulsoup', 'nltk')\n",
      "PREDICTION: ('beautifulsoup', 'web-scraping')\n",
      "\n",
      "How do i get rid of all the smart quotes while parsing a web page?\n",
      "\n",
      "This is my code :\n",
      "name = namestr.decode(\"utf-8\")\n",
      "\n",
      "name.replace(u\"\\u2018\", \"\").replace(u\"\\u2019\", \"\").replace(u\"\\u201c\",\"\").replace(u\"\\u201d\", \"\")\n",
      "\n",
      "This doesn't seem to work . I still find &ldquo , &rdquo etc in my text. Also this text has been parsed using beautiful soup \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('c',)\n",
      "PREDICTION: ('c',)\n",
      "\n",
      "Need a tool to detect the type of variables in C code\n",
      "\n",
      "I am in a project developing specific source-to-source compiler. At this stage, I need to find the type of variables in a C source code. For example, if the code is c[i]=j*f[k]+p; I should find the type of c, i, j, f, k, and p variables (int*, float, and any other type defined in the source). Is there any tool to do so? If there are multiple tools, I prefer a python-based tool.\n",
      "Thank you in ahead.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pygame',)\n",
      "PREDICTION: ('pygame',)\n",
      "\n",
      "pygame is not working\n",
      "\n",
      "I am trying to play a song with pygame and it is not playing the song.\n",
      "My code:\n",
      "import pygame,time\n",
      "pygame.init()\n",
      "print \"Mixer settings\", pygame.mixer.get_init()\n",
      "print \"Mixer channels\", pygame.mixer.get_num_channels()\n",
      "pygame.mixer.music.set_volume(1.0)\n",
      "pygame.mixer.music.load('C:/1.mp3')\n",
      "print \"Play\"\n",
      "pygame.mixer.music.play(0)\n",
      "while pygame.mixer.music.get_busy():\n",
      "   print \"Playing\", pygame.mixer.music.get_pos()\n",
      "time.sleep(1)\n",
      "print \"Done\"\n",
      "\n",
      "I am getting output as\n",
      "Mixer settings (22050, -16, 2)\n",
      "Mixer channels 8\n",
      "Play\n",
      "Done\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ('python-3.x',)\n",
      "\n",
      "TabError in Python 3\n",
      "\n",
      "Given the following interpreter session: \n",
      ">>> def func(depth,width):\n",
      "...   if (depth!=0):\n",
      "...     for i in range(width):\n",
      "...       print(depth,i)\n",
      "...       func(depth-1,width)\n",
      "  File \"<stdin>\", line 5\n",
      "    func(depth-1,width)\n",
      "                  ^\n",
      "TabError: inconsistent use of tabs and spaces in indentation\n",
      "\n",
      "Can someone please tell me what is the TabError in my code?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('dataframe',)\n",
      "PREDICTION: ('dataframe', 'pandas')\n",
      "\n",
      "Multiple IF conditions on dataframe columns in Python\n",
      "\n",
      "I have a simple dataframe with two columns: \"Sex\" and \"Alive\". What I want to do is to count how many of each gender has survived (indicated by 1 in the file). Here's my file:\n",
      "Sex Alive\n",
      "male    1\n",
      "male    0\n",
      "female  1\n",
      "female  1\n",
      "\n",
      "I tried using the following code, but it has not worked. Any hints and pointers is greatly appreciated.\n",
      "for r in df:\n",
      "    if [ (df.Sex=='male') & (df.Alive==1) ]:\n",
      "        male_survival_rate += 1\n",
      "    else:\n",
      "        male_dead_rate += 1\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django', 'django-rest-framework')\n",
      "\n",
      "how to use django-registeration in django framework?\n",
      "\n",
      "I know my question my be simple or basic. But i am asking this because i searched a good article or tutorial so much and didn't find any good detailed article related to django-registration. can any one help me to explain or to give example that how to use this.\n",
      "Note: Please don't mark negatively if you don't like my question.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib',)\n",
      "PREDICTION: ('matplotlib',)\n",
      "\n",
      "Exporting a matplotlib animation to an image\n",
      "\n",
      "I have a matplotlib animation (for instance one of these) and I want to export it to a SVG image where several frames are superimposed (e.g. in this paper, Fig. 2. on the 5th page). The objective is to have an image that describes the motion as clearly as possible.\n",
      "What is the best way to achieve this?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('c++', 'pyqt')\n",
      "PREDICTION: ('pyqt', 'pyqt4')\n",
      "\n",
      "a dialog for throublshooting instead of print command pyqt\n",
      "\n",
      "I have a code and inherited from a Qt class, So :\n",
      "print mystr.encode(\"utf-8\")\n",
      "\n",
      "give me the following error:\n",
      "AttributeError: 'QString' object has no attribute 'encode'\n",
      "\n",
      "So, I decided to use Qt ready dialog, because i know it have same dialog for throublshooting such as alert in JavaScript, My coworker was using in C++ same thing.\n",
      "Question: Which is name of their function?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('tkinter',)\n",
      "PREDICTION: ('tkinter',)\n",
      "\n",
      "How can I disable mouse control of a Tkinter listbox?\n",
      "\n",
      "I was wondering whether it is possible to disable mouse control over a listbox in Tkinter. I want arrow keys navigation only. Is this possible?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('import',)\n",
      "PREDICTION: ('import',)\n",
      "\n",
      "Import programs in the same folder python\n",
      "\n",
      "I'm importing statsmod.py, which is stored in the same folder as the program I am running. I'm importing it like this: \n",
      "import statsmod\n",
      "\n",
      "When I run the program, however, I get the following: \n",
      "raise TypeError(msg.format(type(x).__name__)) from None\n",
      "\n",
      "Am I not importing it correctly?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pygame', 'twisted')\n",
      "PREDICTION: ('pygame',)\n",
      "\n",
      "Network Support for Pygame\n",
      "\n",
      "I am making a simple multiplayer economic game in pygame. It consists of turns of a certain length, at the end of which, data is sent to the central server. A few quick calculations are done on the data and the results are sent back to the players. My question is how I should implement the network support. I was looking at Twisted and at Pyro and any suggestions or advice would be appreciated.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('for-loop', 'regex')\n",
      "PREDICTION: ('regex',)\n",
      "\n",
      "Using REGEX over splited string\n",
      "\n",
      "I'm trying to split a string into sub string, splitting by the 'AND' term, and after that\n",
      "clean each sub string from \"garbage\".   \n",
      "The following code get the error: \n",
      "\n",
      "AttributeError: 'NoneType' object has no attribute 'group'  \n",
      "\n",
      "import re\n",
      "def fun(self, str):\n",
      "    for subStr in str.split('AND'):\n",
      "        p = re.compile('[^\"()]+')\n",
      "        m = p.match(subStr)\n",
      "        print (m.group())\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django',)\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "How to find number of users, number of users with a profile object, and monthly logins in Django\n",
      "\n",
      "Is there an easy way in Django to find the number of Users, Number of Users with profile objects, and ideally number of logins per month (but could do this with Google Analytics). I can see all the data is there in the admin interface, but I'm unsure of how to get to it in Python land.  Has anyone seen any examples of counting number of user objects?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('google-app-engine', 'json', 'python-2.7')\n",
      "PREDICTION: ('google-app-engine',)\n",
      "\n",
      "Python 2.7 on App Engine, simplejson vs native json, who's faster?\n",
      "\n",
      "I've had the understanding that simplejson is much faster than the native json in Python, such as this thread:\n",
      "`json` and `simplejson` module differences in Python\n",
      "However, I was just thrown for a loop when I read in App Engines documentation that with Python 2.7 \n",
      "\n",
      "Uses the native JSON library, which is much faster than simplejson.\n",
      "\n",
      "http://code.google.com/appengine/docs/python/python27/newin27.html\n",
      "So now I'm confused. Everywhere else it seems to say simplejson is better, but now App Engine with Python 2.7 says the native is faster. What gives?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('csv',)\n",
      "PREDICTION: ('csv',)\n",
      "\n",
      "How to count the number of times a word appears in each row of a csv file, in python\n",
      "\n",
      "I have a csv document in which each row contains a string of words. For each row, I want to know how many times each words appears in that row, how would I go about this?\n",
      "Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('file',)\n",
      "PREDICTION: ()\n",
      "\n",
      "Is there a uniform python library to transfer files using different protocols\n",
      "\n",
      "I know there is ftplib for ftp, shutil for local files, what about NFS? I know urllib2 can get files via HTTP/HTTPS/FTP/FTPS, but it can't put files.\n",
      "If there is a uniform library that automatically detects the protocol (FTP/NFS/LOCAL) with URI and deals with file transfer (get/put) transparently, it's even better, does it exist?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ()\n",
      "\n",
      "Geektool not working with python3\n",
      "\n",
      "When I try to run a python script with python3 it does not work but it works when I just use python. Why is this?\n",
      "I have a simple hello.py file:\n",
      "__author__ = 'A'\n",
      "print(\"hellow\")\n",
      "\n",
      "When I use python ~/path/hello.py with geektool it works, but not with python3 ~/path/hello.py, the same works from terminal.\n",
      "Also, where can I see geektool's log file?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('list',)\n",
      "PREDICTION: ('list',)\n",
      "\n",
      "List Changing In Python\n",
      "\n",
      "[(946871.62999999884, [303.91000000000003]), (964918.62999999884, [353.75]),  \n",
      " (965117.81999999878, [356.98000000000002]), (959944.86999999883, [528.53999999999996])]\n",
      "\n",
      "I have a list that looks like this. I was wondering if there was any way to remove the minilist, so that it would look like \n",
      "[(946871.62999999884, 303.91000000000003), (964918.62999999884, 353.75),  \n",
      " (965117.81999999878, 356.98000000000002), (959944.86999999883, 528.53999999999996)]\n",
      "\n",
      "Thanks for the help!\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib', 'pandas')\n",
      "PREDICTION: ('matplotlib',)\n",
      "\n",
      "plot legends of a correlation matrix\n",
      "\n",
      "I have a correlation mat obtained from a data frame\n",
      ">>> mat\n",
      "                   Lcaud     Rcaud  Left_cereb_gm  Right_cereb_gm     Lamyg            \n",
      "Rcaud           0.931934  1.000000       0.856891        0.715523  0.924995\n",
      "Left_cereb_gm   0.915274  0.856891       1.000000        0.938301  0.601521\n",
      "Right_cereb_gm  0.744007  0.715523       0.938301        1.000000  0.445450\n",
      "Lamyg           0.754676  0.924995       0.601521        0.445450  1.000000\n",
      "Rput            0.717757  0.876985       0.635881        0.462773  0.912815\n",
      "\n",
      "I can nicely plot it with : \n",
      "heatmap = plt.pcolor(mat, cmap=matplotlib.cm.Blues)\n",
      "\n",
      "but I would like to get the legends 'Lcaud', 'Rcaud' etc... on the plot. \n",
      "How can I achieve this?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('java', 'machine-learning')\n",
      "PREDICTION: ()\n",
      "\n",
      "Machine learning on single node with Bid Mach\n",
      "\n",
      "Wonderinf if anybody has experienced configuring and using BidMach on single machine for machine learning ?\n",
      "Did you get any differences in usage with H2O ?\n",
      "What about integration with CUDA API ? (does it use CuDNN ?)\n",
      "tahnks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('opencv',)\n",
      "PREDICTION: ('opencv',)\n",
      "\n",
      "Hough circle detection: Blurring the image before calling hough circle algorithm?\n",
      "\n",
      "http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_houghcircles/py_houghcircles.html\n",
      "In the example here, I do not know why it called the median filter first before the hough circle algorithm. Is this meant to give better detection?\n",
      "Also, is there any other tricks in general that maybe useful when calling Hough circle algorithm? In particular, if the circular object has same brightness with its background, hence looking homogeneous on the grey scale, is there anything I can do here?\n",
      "Also, if I cannot get perfect detection, I would prefer having less detected circles but rather the detected ones are correct.\n",
      "Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('flask', 'opencv')\n",
      "PREDICTION: ('flask',)\n",
      "\n",
      "How to pass image to python script using flask?\n",
      "\n",
      "I have a website (managed with python-flask) with images on canvas and i would like to pass the content of those canvas to another python script as images. \n",
      "The other python script is using openCV in order to perform face detection.\n",
      "I know i could upload the image on my server and then read the file on my opencv application but i would like not to save any data on my server.\n",
      "Do you have any ideas ?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('pip', 'ubuntu')\n",
      "PREDICTION: ('linux', 'ubuntu')\n",
      "\n",
      "Could not run hg on ubuntu terminal\n",
      "\n",
      "I installed mercurial using sudo apt-get install mercurial and it installed properly. But when I run any hg command I get the following Error Message- \n",
      "\n",
      "abort: couldn't find mercurial libraries in [/home/yashu/bin\n",
      "  /usr/local/lib/python3.4/dist-packages/networkx-2.0.dev_20150923032326-py3.4.egg\n",
      "  /usr/local/lib/python3.4/dist-packages/decorator-4.0.2-py3.4.egg\n",
      "  /usr/local/lib/python3.4/dist-packages/Cython-0.23.2-py3.4-linux-x86_64.egg\n",
      "  /usr/local/lib/python3.4/dist-packages/pgmpy-0.1.0-py3.4.egg\n",
      "  /usr/local/lib/python3.4/dist-packages/pip-1.2.1-py3.4.egg\n",
      "  /usr/lib/python3.4 /usr/lib/python3.4/plat-x86_64-linux-gnu\n",
      "  /usr/lib/python3.4/lib-dynload /usr/local/lib/python3.4/dist-packages\n",
      "  /usr/lib/python3/dist-packages] (check your install and PYTHONPATH)\n",
      "\n",
      "then I tried installing it with pip and it again installed successfully but on runnning any hg command I get the same error message.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('html',)\n",
      "PREDICTION: ('python-requests',)\n",
      "\n",
      "how to get only the <title> with requests module\n",
      "\n",
      "I have a lot of web pages that I just want the title from it, but requests module load the entire HTML and that seems to take a while \n",
      "how to get only the title\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('java',)\n",
      "PREDICTION: ()\n",
      "\n",
      "Data type to save a table\n",
      "\n",
      "I want to save a table contain records with some attribute, \n",
      "for example\n",
      "Name gender Age \n",
      "----------------\n",
      "Josh Male   22\n",
      "\n",
      "so I can easily access all records and change the attribute,\n",
      "what is the easiest way to save this data ? ,\n",
      "in C++ I use double vector.\n",
      "if you have solution in python or java please tell me :)\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('image', 'matplotlib')\n",
      "PREDICTION: ()\n",
      "\n",
      "Save raw data as tif\n",
      "\n",
      "I need to analyze a part of an image, selected as a submatrix, in a tif file. I would like to have the image in raw format, with no frills (scaling, axis, labels and so on)... How could I do that?\n",
      "This is the code I am using now:\n",
      " submatrix = im[x_min:x_max, y_min:y_max]\n",
      " plt.imshow(submatrix)\n",
      " plt.savefig(\"subplot_%03i_%03i.tif\" % (index, peak_number), format = \"tif\")\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('datetime',)\n",
      "PREDICTION: ('datetime',)\n",
      "\n",
      "Python doesn't compare date?\n",
      "\n",
      "I have this launch_time:\n",
      "\n",
      "2015-01-15 10:31:54+00:00\n",
      "\n",
      "I get current_time\n",
      "current_time = datetime.datetime.now(launch_time.tzinfo)\n",
      "\n",
      "I want both of times are same so I used tzinfo. So, the value of current_time is\n",
      "\n",
      "2015-01-16 10:55:50.200571+00:00\n",
      "\n",
      "I take the running time with this:\n",
      "running_time = (current_time - launch_time).seconds/60\n",
      "\n",
      "The value return only 23 minutes. It should be one day + 23 minutes = 1463 minutes\n",
      "Can someone help me. Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('numpy',)\n",
      "PREDICTION: ('list', 'numpy')\n",
      "\n",
      "collapse a list of ndarray to a matrix\n",
      "\n",
      "I have a list containing objects of type numpy.ndarray, all list elements has same .shape value.\n",
      "How can I collapse this into a matrix?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('django', 'image')\n",
      "PREDICTION: ('django',)\n",
      "\n",
      "Modify image size in Django\n",
      "\n",
      "I have ImageField in my model. Because of people may want to upload big photos. Now I want to modify picture size after uploading as it is on Facebook or other websites which have profile pictures.\n",
      "this is my ImageField description\n",
      "avatar = models.ImageField(upload_to=avatar_directory_path, null=True, blank=True)\n",
      "\n",
      "and this is avatar_directory_path function\n",
      "def avatar_directory_path(instance, filename):\n",
      "\n",
      "    return 'avatars/{0}/{1}'.format(instance.user.id, filename)\n",
      "\n",
      "What can you suggest me ?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('numpy',)\n",
      "PREDICTION: ('matplotlib',)\n",
      "\n",
      "logarithmic spaced values below 1\n",
      "\n",
      "I want to plot values, in logarithmic scale, over the range 0.001 - 1000 (x axis); when I try:\n",
      "import numpy as np\n",
      "x = np.logspace(0.001, 1000, 11) \n",
      "\n",
      "I cannot get any values below 1; is there another way I can create logarithmic spaced values below 1 (and above 0.001) \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('list',)\n",
      "PREDICTION: ('list',)\n",
      "\n",
      "Re-list a item from a nested list\n",
      "\n",
      "I'm trying to figure how to pick all the fruits like:\n",
      "[['id', 'sub type', 'type'], ['1', 'apples', 'fruit'], ['15', 'orange', 'fruit'], ['3', 'corn', 'vegtable']]\n",
      "\n",
      "How do I output:\n",
      "['sub type','apple','orange','corn']\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('string',)\n",
      "PREDICTION: ()\n",
      "\n",
      "increasing string size through loop\n",
      "\n",
      "what's a simple way to increase the length of a string to an arbitrary integer x? like 'a' goes to 'z' and then goes to 'aa' to 'zz' to 'aaa', etc. \n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('java',)\n",
      "PREDICTION: ('windows',)\n",
      "\n",
      "Installing JDK and Python and setting the PATH variable on Windows 7\n",
      "\n",
      "I recently installed Java's JDK and Python in my Windows 7 system. I wanted to access both programs from the command line (whether it be cmd or cygwin) so I used the PATH global variable and entered the path to my JDK. What can I do so that python and JDK are accessed by PATH? What I am doing now is changing the PATH variable every time.\n",
      "Thanks\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('python-3.x',)\n",
      "PREDICTION: ('module',)\n",
      "\n",
      "Is there an easy way to get all common module extensions?\n",
      "\n",
      "I am making a library that deals with Python modules.  Without getting into details, I need a list of the common Python module extensions.\n",
      "Obviously, I want .py, but I'd also like to include ones such as .pyw, .pyd, etc.  In other words, I want anything that you can import.\n",
      "Is there a tool in the standard library which will make this list for me?  Or do I have to make it myself (and hardcode all of the values)?\n",
      "extensions = ['.py', '.pyw', ...]\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('multiprocessing',)\n",
      "PREDICTION: ('multiprocessing',)\n",
      "\n",
      "python multiprocessing manager\n",
      "\n",
      "My problem is:\n",
      "I have 3 procs that would like to share config loaded from the same class and a couple of queues. I would like to spawn another proc as a multiprocessing.manager to share those informations. \n",
      "How can I do that? Could someone purchase a sample code avoiding use of global vars and making use of multiprocessing manager class?\n",
      "Python docs wasn't so helpfull :-(\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('windows',)\n",
      "PREDICTION: ('windows',)\n",
      "\n",
      "How to make Python accessible via CMD prompt without full path?\n",
      "\n",
      "I have Python installed on D:\\python27. Now I want to kno how to make it so that I don't have to type\n",
      "D:\\python27\\python.exe myscript.py\n",
      "\n",
      "but instead, just use\n",
      "python myscript.py\n",
      "\n",
      "I know that this should already be configured after the install but somehow, the laptop I'm using didn't register the python shortcut. So how do I set it manually?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('arrays',)\n",
      "PREDICTION: ('list',)\n",
      "\n",
      "Finding length of items from a list\n",
      "\n",
      "I have two list in a python\n",
      "list1=['12aa','2a','c2']\n",
      "\n",
      "list2=['2ac','c2a','1ac']\n",
      "\n",
      "First- Finding combinations of each two item from list1.\n",
      "Second- Finding combinations of each two item from list2.\n",
      "Third- Finding combinations of each two items from list1 and list2\n",
      "Fourth- Calculating each combinations total length\n",
      "Advice and help in Python is appreciated.\n",
      "Thank you\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('dictionary',)\n",
      "PREDICTION: ('dictionary',)\n",
      "\n",
      "Python: Finding the value of a key with a variable assigned that key\n",
      "\n",
      "This is worded terribly but I'm very new to coding, here is an example of what I want to do:  \n",
      "import random  \n",
      "dict={\n",
      "'option1': 1,\n",
      "'option2': 2,\n",
      "}\n",
      "randomDictionaryChoice=random.choice(list(dict.keys()))\n",
      "valueOfThatKey=dict.get('randomDictionaryChoice')\n",
      "print(valueOfThatKey)\n",
      "\n",
      "At the moment it prints \"none\" instead of 1 or 2.  \n",
      "This is likely structured badly or bad ettiquette so feel free to comment on that too. Thanks in advance.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "TRUE: ('matplotlib',)\n",
      "PREDICTION: ('matplotlib', 'osx')\n",
      "\n",
      "Unable to modify file name when saving matplotlib figures in Mac system\n",
      "\n",
      "After executing the show() command in Python, I can only save figures as figure_1.png\n",
      "I cannot change my file name. No typing is allowed next to \"Save As:\". How to enable typing my filename at this point?\n",
      "I apologize that I need at least 10 reputations to post screenshot.\n",
      "\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([X_val_b, X_val_t])\n",
    "\n",
    "l_pred = label_encoder.inverse_transform(binarize_model_output(predictions, threshold=opt_thres))\n",
    "l_true = label_encoder.inverse_transform(y_val)\n",
    "raw_texts = validation_questions[content_field]\n",
    "titles = validation_questions[\"Title\"]\n",
    "\n",
    "for pred, act, raw_txt, title in zip(l_pred, l_true, raw_texts, titles):\n",
    "    print(f\"TRUE: {act}\\nPREDICTION: {pred}\\n\")\n",
    "    print(f\"{title}\\n\")\n",
    "    print(raw_txt)\n",
    "    print(f\"--------------------------\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

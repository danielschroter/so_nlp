{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline using Title and Body Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from toolbox.data_prep_helpers import *\n",
    "from toolbox.evaluation import *\n",
    "\n",
    "from models.lstm_classifier import create_model\n",
    "from models.title_body_lstm import create_model as tb_create_model\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pythonquestions/\"\n",
    "ft_path = \"alldata.ft\"  # set this to None if you want to train your own fasttext embeddings\n",
    "n_top_labels = 100\n",
    "n_epochs = 20\n",
    "max_question_words = 100\n",
    "sample_size = 10000  # set to -1 to use entire data\n",
    "normalize_embeddings = True\n",
    "use_titles = False\n",
    "\n",
    "tokenized_field = \"q_all_body_tokenized\"\n",
    "content_field = \"Body_q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(data_path, ignore_cache=False, tokenized_field=tokenized_field, content_field=content_field)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(sample_size) if sample_size > 0 else df\n",
    "del df\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_html_tags(chunk, [\"Body_q\"])\n",
    "#print(f\"{i}: generating question level tokens\")\n",
    "sample[\"q_title_tokenized\"] = sample[\"Title\"].apply(generate_question_level_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have some nans in our tags which break target encoding\n",
    "print(sample.shape)\n",
    "sample = sample[sample[\"tags\"].apply(lambda tags: all([isinstance(t, str) for t in tags]))]\n",
    "print(sample.shape)\n",
    "\n",
    "\n",
    "# Reduce the number of tags and adjust dataframe accordingly\n",
    "sample = reduce_number_of_tags(sample, n_top_labels)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"tags\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text into words on question level\n",
    "data = sample[sample[tokenized_field].apply(len) <= max_question_words]\n",
    "train_data, test_data = train_test_split(data, test_size = 0.2)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word embeddings ONLY with training data\n",
    "# wv = create_Word2Vec_embeddings(train_data, \"Body_q\")\n",
    "# Use FastText to include solution for out-of-vocab words\n",
    "if ft_path is not None:\n",
    "    wv = load_fasttext_embeddings(ft_path)\n",
    "else:\n",
    "    wv = create_FastText_embedding(train_data, content_field)\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Title and Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = train_data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_train_b = train_data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "\n",
    "padding_element = np.array([0.0] * X_train_t.iloc[0].shape[-1])\n",
    "\n",
    "X_train_t_padded = pad_sequences(X_train_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_train_b_padded = pad_sequences(X_train_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_train_t_padded.shape)\n",
    "print(X_train_b_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = test_data[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_test_b = test_data[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "\n",
    "X_test_t_padded = pad_sequences(X_test_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_test_b_padded = pad_sequences(X_test_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_test_t_padded.shape)\n",
    "print(X_test_b_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = MultiLabelBinarizer()\n",
    "label_encoder.fit(train_data[\"tags\"])\n",
    "y_train = label_encoder.transform(train_data[\"tags\"])\n",
    "y_test = label_encoder.transform(test_data[\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tb_create_model(embedding_dim=100, output_dim=100, mask_value=0.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "model.fit(x=[X_train_t_padded, X_train_b_padded], y=y_train, batch_size=32, epochs=n_epochs, validation_data=[[X_test_t_padded, X_test_b_padded], y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_predictions = 100\n",
    "\n",
    "predictions = model.predict([X_test_t_padded, X_test_b_padded])\n",
    "\n",
    "l_pred = label_encoder.inverse_transform(binarize_model_output(predictions, threshold=0.10))\n",
    "l_true = label_encoder.inverse_transform(y_test)\n",
    "texts = test_data[tokenized_field]\n",
    "raw_texts = test_data[content_field]\n",
    "titles = test_data[\"Title\"]\n",
    "\n",
    "for pred, act, txt, raw_txt, title in zip(l_pred, l_true, texts, raw_texts, titles):\n",
    "    print(f\"TRUE: {act}\\nPREDICTION: {pred}\\n\")\n",
    "    print(f\"{title}\\n----------\")\n",
    "    print(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_t_padded, X_test_b_padded], batch_size=64)\n",
    "output_evaluation(model, sample_size, max_question_words, n_top_labels, y_train, predictions, normalize_embeddings, 1, None, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

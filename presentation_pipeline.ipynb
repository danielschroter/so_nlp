{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Stack-Overflow Questions\n",
    "\n",
    "**The data**\n",
    "* Python questions from Stackoverflow: [https://www.kaggle.com/stackoverflow/pythonquestions](https://www.kaggle.com/stackoverflow/pythonquestions)\n",
    "* ~ 600000 questions\n",
    "* each question with 0-5 tags\n",
    "\n",
    "**The problem**\n",
    "\n",
    "Can we predict tags from question / title texts? If so, how well?\n",
    "\n",
    "**Approach**\n",
    "\n",
    "Create several models and compare performances:\n",
    "* Bag-of-words model\n",
    "* sequential LSTM model for question bodies\n",
    "* *composite LSTM model for question bodies + titles*   <== this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pythonquestions/\"\n",
    "ft_path = \"alldata.ft\"  # set this to None if you want to train your own fasttext embeddings\n",
    "n_top_labels = 100  # number of top labels to reduce dataset to\n",
    "max_question_words = 100\n",
    "sample_size = 1000  # set to -1 to use entire dataset\n",
    "normalize_embeddings = True  # whether to normalize fasttext embeddings between -1, +1\n",
    "\n",
    "tokenized_field = \"q_title_tokenized\" if use_titles else \"q_all_body_tokenized\"\n",
    "content_field = \"Title\" if use_titles else \"Body_q\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.data_prep_helpers import load_data\n",
    "\n",
    "df = load_data(\"presentation_sample.pkl\", ignore_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slim down number of tags\n",
    "\n",
    "We remove all tags that are not within the top *n_top_label* tags of the dataset. Afterwards, we remove any row that has no tags left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.data_prep_helpers import reduce_number_of_tags\n",
    "\n",
    "df = reduce_number_of_tags(df, n_top_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Body_q\"].iloc[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.data_prep_helpers import remove_html_tags\n",
    "\n",
    "# question bodies are stored as html code, we need to extract the content only\n",
    "remove_html_tags(df, [\"Body_q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Body_q\"].iloc[100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We need to tokenize questions in order to be able to apply/train embeddings on them.\n",
    "\n",
    "To do this, we use the word_tokenize function from the nltk library ([https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html)) to transform multiple sentences of a question to a 1-dimensional list of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization example\n",
    "generate_question_level_tokens(\"Please help! How do I format in markdown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.data_prep_helpers import generate_question_level_tokens\n",
    "\n",
    "df[\"q_body_tokenized\"] = df[\"Body_q\"].apply(generate_question_level_tokens)\n",
    "df[\"q_title_tokenized\"] = df[\"Title\"].apply(generate_question_level_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samples with too many tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove questions that contain more than max_questions_words words to meet memory limitations. \n",
    "df = df[df[\"q_body_tokenized\"].apply(len).between(1, max_question_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText word embeddings\n",
    "\n",
    "We trained our own embeddings, because Code is often related to certain key words.\"Pandas\" for instance is related to the python library. Hence it's meaning within python code is totally different from it's meaning in pretrained embeddings.\n",
    "\n",
    "\n",
    "Why FastText?\n",
    "\n",
    "FastText includes reasonable mechanisms to deal with words, where no embedding exists. \n",
    "It represents a word as a bag of character n-grams. For words, which are out of vocab it calculates the embedding by combining the specific n-grams. For Training we used skip gram and cbow, where xy turned out to have a better perfomance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word embeddings ONLY with training data\n",
    "# wv = create_Word2Vec_embeddings(train_data, \"Body_q\")\n",
    "# Use FastText to include solution for out-of-vocab words\n",
    "if ft_path is not None:\n",
    "    wv = load_fasttext_embeddings(ft_path)\n",
    "else:\n",
    "    wv = create_FastText_embedding(train_data, content_field)\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = df[\"q_title_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))\n",
    "X_b = df[\"q_all_body_tokenized\"].apply(lambda x: np.array([wv.word_vec(w, use_norm=normalize_embeddings) for w in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to model-compatible format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad question embeddings to equal length to unify tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padding_element = np.array([0.0] * X_train_t.iloc[0].shape[-1])\n",
    "\n",
    "X_t = pad_sequences(X_t, padding=\"post\", dtype='float32', value=padding_element)\n",
    "X_b = pad_sequences(X_b, padding=\"post\", dtype='float32', value=padding_element)\n",
    "print(X_t.shape)\n",
    "print(X_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target data\n",
    "With the MultiLabelBinarizer we create a (sample x label) matrix where for each record a 1 represents the presence of a certain label and a 0 its absence. (Similar to one-hot-encoding for single class problems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "label_encoder.fit(df[\"tags\"])\n",
    "y = label_encoder.transform(df[\"tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our title/body model takes title and body token sequences as separate inputs. These inputs are each passed through a masking layer which allows following layers (i.e. the lstm layers) to skip padding elements in the sequence. The masked inputs are processed by two separate lstm layers, whos last output vectors are concatenated to form one big context vector. This context is then passed through a fully connected layer with a sigmoid activation function, which assigns \"independent\" probabilities to each output class.\n",
    "\n",
    "The model is visualized in the diagram below. For this visual example, we go with the following properties:\n",
    "* batch size: 32\n",
    "* sequence length: 50\n",
    "* embedding size: 100\n",
    "* lstm size (each): 64\n",
    "\n",
    "![model architecture](graphics/title_body_model.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct GridSearch to find \"optimal\" hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.training import grid_search_es\n",
    "\n",
    "search_params = {\n",
    "    # \"lstm_layer_size\": [512, 256, 128],\n",
    "    # \"lstm_dropout\": [0.0, 0.2, 0.4],\n",
    "    \"lstm_layer_size\": [16],\n",
    "    \"lstm_dropout\": [0.0],\n",
    "    # don't change these:\n",
    "    \"output_dim\": [y.shape[-1]]\n",
    "}\n",
    "\n",
    "all_hists = grid_search_es(create_model, search_params)\n",
    "\n",
    "best_params, best_hist, best_loss = min(all_hists, key=lambda x: x[2])\n",
    "\n",
    "epoch_lengths = [len(h[\"val_loss\"]) for h in best_hist]\n",
    "print(f\"best combindation: {best_params}\")\n",
    "print(f\"avg min val_loss: {best_loss} -- epoch counts: {epoch_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.title_body_lstm import create_model\n",
    "\n",
    "model = create_model(**best_params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "import datetime\n",
    "\n",
    "model_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir=\"logs/fit/\" + model_name\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, verbose=0),\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    ModelCheckpoint(filepath=f\"checkpoints/{model_name}\", monitor=\"val_loss\", restore_best_weights=True, verbose=0)\n",
    "]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, validation_data=[X_test, y_test], callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_predictions = 100\n",
    "\n",
    "predictions = model.predict([X_test_t_padded, X_test_b_padded], batch_size=64)\n",
    "\n",
    "l_pred = label_encoder.inverse_transform(binarize_model_output(predictions, threshold=0.10))\n",
    "l_pred_out = l_pred[:n_predictions]\n",
    "l_true = label_encoder.inverse_transform(y_test[:n_predictions])\n",
    "texts = test_data[tokenized_field][:n_predictions]\n",
    "raw_texts = test_data[content_field][:n_predictions]\n",
    "titles = test_data[\"Title\"][:n_predictions]\n",
    "\n",
    "for pred, act, txt, raw_txt, titles in zip(l_pred, l_true, texts, raw_texts, titles):\n",
    "    print(f\"TRUE: {act}\\nPREDICTION: {pred}\\n\")\n",
    "    print(f\"{title}\\n\")\n",
    "    print(f\"{raw_txt}\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1_Micro Score Optimization\n",
    "\n",
    "F1_Score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "For Multi-Labeling we used the F1_Micro score which calculates the number of \"True Positives\", \"False Positives\" and \"False Negatives\" globally.\n",
    "As we use the sigmoid function within our model we get values between 0 and 1 for every label. Hence it is necessary to define a threshold to decide whether a certain label is predicted (=1). The threshold, that maximizes the f1_micro score is calculated within the output_evaluation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_evaluation(model, sample_size, max_question_words, n_top_labels, y_test, predictions, normalize_embeddings, None, None, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
